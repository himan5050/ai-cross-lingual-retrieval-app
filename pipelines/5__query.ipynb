{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2896f8a",
   "metadata": {},
   "source": [
    "# <b>Query processing pipeline using indexed and matadata</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2228b3c",
   "metadata": {},
   "source": [
    "#### 1. Setup: load model, tokenizer, FAISS index, and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25b2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, torch, faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318a7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model (speed tiers):\n",
    "# FAST:    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" (384d)\n",
    "# BALANCE: \"intfloat/multilingual-e5-base\" (768d)\n",
    "# QUALITY: \"intfloat/multilingual-e5-large\" (1024d)\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0c574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TAG = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "METADATA_PATH = f\"../shared-data-library/metadata/{MODEL_TAG}__meta.parquet\"\n",
    "FAISS_INDEX_PATH  = f\"../shared-data-library/indexes/faiss/{MODEL_TAG}__indexes.faiss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7069184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load meta and FAISS Indexes ---\n",
    "metadata  = pd.read_parquet(METADATA_PATH)\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b10c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model / tokenizer ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE  = torch.float16 if DEVICE.type == \"cuda\" else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=(torch.float16 if DEVICE.type==\"cuda\" else None)).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bb9e4",
   "metadata": {},
   "source": [
    "## <b>Functions.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_token_budget(tok, headroom=16, cap_default=512):\n",
    "    m = getattr(tok, \"model_max_length\", None)\n",
    "    if m is None or m > 100_000_000: m = cap_default\n",
    "    return max(32, int(m - headroom))\n",
    "TOKEN_BUDGET = model_token_budget(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75898dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5 uses \"query: \" prefix. Others don't.\n",
    "def add_query_prefix(texts):\n",
    "    return [f\"query: {t}\" for t in texts] if \"intfloat/multilingual-e5\" in MODEL_NAME.lower() else texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "    return (last_hidden_state * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9288e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize_np(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56dddc",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39735f75",
   "metadata": {},
   "source": [
    "### 1. Batch embed queries (fast, safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_queries(queries: list[str], batch_size: int = 128) -> np.ndarray:\n",
    "    \"\"\"Return (Q, d) L2-normalized float32 embeddings for queries.\"\"\"\n",
    "    vecs = []\n",
    "    queries = add_query_prefix([str(q) for q in queries])\n",
    "    for i in tqdm(range(0, len(queries), batch_size), desc=\"Embedding queries\", unit=\"batch\"):\n",
    "        batch = queries[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=TOKEN_BUDGET, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(DEVICE, non_blocking=True) for k, v in enc.items()}\n",
    "        with torch.inference_mode(), (\n",
    "            torch.autocast(device_type=DEVICE.type, dtype=torch.float16) if DEVICE.type==\"cuda\" else torch.no_grad()\n",
    "        ):\n",
    "            out    = model(**enc)\n",
    "            pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "            pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)   # cosine ‚Üí IP\n",
    "        vecs.append(pooled.to(torch.float32).cpu().numpy())\n",
    "    return np.vstack(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb378f1",
   "metadata": {},
   "source": [
    "### 2. Dense search (FAISS) for a batch of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_search_batch(q_vecs: np.ndarray, top_k: int = 10) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    q_vecs: (Q, d) float32, L2-normalized\n",
    "    returns (scores, indices) each (Q, top_k) with cosine scores (via inner product).\n",
    "    \"\"\"\n",
    "    assert q_vecs.dtype == np.float32\n",
    "    D, I = index.search(q_vecs, top_k)\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad58d1",
   "metadata": {},
   "source": [
    "### 3. Assemble results into a tidy DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fe701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_df(queries: list[str], D: np.ndarray, I: np.ndarray, meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a long DataFrame with one row per (query, hit).\n",
    "    Columns: query_id, query, rank, dense_score, global_chunk_id, doc_id, chunk_id, title, site, lang, preview, ...\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for qid, (scores, idxs) in enumerate(zip(D, I)):\n",
    "        m = meta.iloc[idxs].copy()\n",
    "        m = m.assign(\n",
    "            query_id = qid,\n",
    "            query    = queries[qid],\n",
    "            rank     = np.arange(1, len(idxs)+1),\n",
    "            dense_score = scores\n",
    "        )\n",
    "        rows.append(m)\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Ensure IDs exist\n",
    "    if \"global_chunk_id\" not in df.columns:\n",
    "        df[\"global_chunk_id\"] = df[\"doc_id\"].astype(str) + \":\" + df[\"chunk_id\"].astype(int).astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c359c8",
   "metadata": {},
   "source": [
    "### 4. Optional: roll up chunk ‚Üí document (max score per doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be368fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_level(df_hits: pd.DataFrame, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep the best chunk per document for each query; then take top_k docs.\n",
    "    \"\"\"\n",
    "    # best chunk per (query_id, doc_id)\n",
    "    best = (df_hits.sort_values([\"query_id\",\"doc_id\",\"dense_score\"], ascending=[True, True, False])\n",
    "                 .groupby([\"query_id\",\"doc_id\"], as_index=False)\n",
    "                 .first())\n",
    "    # rerank per query by score\n",
    "    best[\"rank\"] = best.groupby(\"query_id\")[\"dense_score\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    best = best.sort_values([\"query_id\",\"rank\"]).groupby(\"query_id\").head(top_k).reset_index(drop=True)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811252a3",
   "metadata": {},
   "source": [
    "### 5. One call: run a list of queries end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dense_pipeline(queries: list[str], top_k_chunks: int = 10, top_k_docs: int | None = None):\n",
    "    # 1) embed\n",
    "    q_vecs = embed_queries(queries)\n",
    "    q_vecs = q_vecs.astype(\"float32\")\n",
    "    q_vecs = l2_normalize_np(q_vecs)  # already normalized above; keep for safety\n",
    "\n",
    "    # 2) search\n",
    "    D, I = faiss_search_batch(q_vecs, top_k=top_k_chunks)\n",
    "\n",
    "    # 3) assemble\n",
    "    df_chunks = results_df(queries, D, I, metadata)\n",
    "\n",
    "    # 4) optional doc-level rollup\n",
    "    df_docs = doc_level(df_chunks, top_k=top_k_docs) if top_k_docs else None\n",
    "    return df_chunks, df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6607af",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdf827",
   "metadata": {},
   "source": [
    "#### Run example of query batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fd1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(\"../shared-data-library/ground/queries_sample.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22de35d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child immunization reduces mortality',\n",
       " 'pol√≠tica de vacunaci√≥n infantil',\n",
       " '‡§¨‡§æ‡§≤ ‡§ü‡•Ä‡§ï‡§æ‡§ï‡§∞‡§£ ‡§î‡§∞ ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§¶‡§∞',\n",
       " 'ÂÑøÁ´•ÂÖçÁñ´Êé•Áßç ÂØπ Ê≠ª‰∫°Áéá ÁöÑ ÂΩ±Âìç',\n",
       " 'impact of clean water access on diarrhea']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = queries_df['query_text'].tolist()\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hits, df_docs = run_dense_pipeline(queries, top_k_chunks=10, top_k_docs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a2619",
   "metadata": {},
   "source": [
    "### Save Query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_queries = df_docs[[\"query_id\",\"query\",\"rank\",\"dense_score\",\"doc_id\",\"chunk_id\",\"title\",\"lang\",\"preview\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed file\n",
    "OUT_PARQ  = Path(f\"../shared-data-library/queries/{MODEL_TAG}__dense-query-results.parquet\")\n",
    "df_processed_queries.to_parquet(OUT_PARQ, index=False, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2a68d",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90ea0e",
   "metadata": {},
   "source": [
    "#### <b>Keyword bases query processing.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09155c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"DfZP9TzO\")   # üëà add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4194b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query_str: str, k: int = 50) -> pd.DataFrame:\n",
    "    body = {\n",
    "      \"size\": k,\n",
    "      \"query\": { \"multi_match\": {\n",
    "        \"query\": query_str,\n",
    "        \"fields\": [\"title^2\", \"chunk_text\"]\n",
    "      }}\n",
    "    }\n",
    "    res = es.search(index=\"passages_bm25\", body=body)\n",
    "    rows = []\n",
    "    for rank, hit in enumerate(res[\"hits\"][\"hits\"], start=1):\n",
    "        src = hit[\"_source\"]\n",
    "        rows.append({\n",
    "            \"query\": query_str,\n",
    "            \"rank\": rank,\n",
    "            \"sparse_score\": hit[\"_score\"],\n",
    "            \"global_chunk_id\": src[\"global_chunk_id\"],\n",
    "            \"doc_id\": src[\"doc_id\"],\n",
    "            \"chunk_id\": src[\"chunk_id\"],\n",
    "            \"title\": src.get(\"title\",\"\"),\n",
    "            \"site\": src.get(\"site\",\"\"),\n",
    "            \"lang\": src.get(\"lang\",\"\"),\n",
    "            \"preview\": src.get(\"preview\",\"\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f1f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search_batch(queries: list[str], k: int = 50) -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    for qid, q in enumerate(queries):\n",
    "        df = bm25_search(q, k=k)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df.insert(0, \"query_id\", qid)\n",
    "        all_rows.append(df)\n",
    "    return pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea06ff",
   "metadata": {},
   "source": [
    "#### <b>4. Convert chunk results ‚Üí doc results (best chunk per doc) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fbb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_doc_level(df_hits: pd.DataFrame, top_k_docs: int = 10) -> pd.DataFrame:\n",
    "    # keep best chunk per (query_id, doc_id)\n",
    "    sub = (df_hits\n",
    "           .sort_values([\"query_id\",\"doc_id\",\"sparse_score\"], ascending=[True, True, False])\n",
    "           .groupby([\"query_id\",\"doc_id\"], as_index=False)\n",
    "           .first())\n",
    "    # rerank docs per query\n",
    "    sub[\"rank\"] = sub.groupby(\"query_id\")[\"sparse_score\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    sub = sub.sort_values([\"query_id\",\"rank\"]).groupby(\"query_id\").head(top_k_docs).reset_index(drop=True)\n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a3f65",
   "metadata": {},
   "source": [
    "### <b> 5. Run the baseline for a list of queries.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba1b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: load from CSV with mapping (query_id, query_text)\n",
    "# queries_df = pd.read_csv(\"data/queries.csv\")\n",
    "# queries = queries_df[\"query_text\"].tolist()\n",
    "df_sparse_chunks = bm25_search_batch(queries, k=5)\n",
    "df_sparse_docs   = bm25_doc_level(df_sparse_chunks, top_k_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd22e6",
   "metadata": {},
   "source": [
    "### Save query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3bce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_sparse_queries = df_sparse_docs[[\"query_id\",\"query\",\"rank\",\"sparse_score\",\"doc_id\",\"chunk_id\",\"title\",\"lang\",\"preview\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce899457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed file\n",
    "OUT_PARQ  = Path(f\"../shared-data-library/queries/{MODEL_TAG}__sparse-query-results.parquet\")\n",
    "df_processed_sparse_queries.to_parquet(OUT_PARQ, index=False, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b1ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
