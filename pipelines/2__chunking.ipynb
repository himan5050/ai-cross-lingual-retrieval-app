{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1891395",
   "metadata": {},
   "source": [
    "# <b>Preprocessed document chunking and embedding pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b3598",
   "metadata": {},
   "source": [
    "#### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d70ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "nb_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868333a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import math, time, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# If you need sentence splitting fallback:\n",
    "try:\n",
    "    from blingfire import text_to_sentences\n",
    "except Exception:\n",
    "    text_to_sentences = None\n",
    "\n",
    "# Hugging Face tokenizer for accurate token counts (choose your embedding model)\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee7ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configure your paths here ----\n",
    "\n",
    "# processed file\n",
    "OUT_PARQ  = Path(\"../shared-data-library/out/df_passages.parquet\")\n",
    "\n",
    "# e.g., Path(\"../shared-data-library/out/df_processed.parquet\") if you also want JSONL\n",
    "OUT_JSONL = None\n",
    "\n",
    "# 2k–10k works well; adjust to memory/CPU\n",
    "BATCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f8e15",
   "metadata": {},
   "source": [
    "#### 2) Choose a tokenizer (match your embedding model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82793bf",
   "metadata": {},
   "source": [
    "Pick the tokenizer that matches the embedding model you’ll use in retrieval (so chunk sizes reflect the true token budget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e495aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenizer that matches your embedding model:\n",
    "# Examples:\n",
    "# TOKENIZER_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "TOKENIZER_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# TOKENIZER_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ee3b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_token_budget(tok, headroom=16, cap_default=512):\n",
    "    \"\"\"\n",
    "    Returns a safe per-chunk token budget:\n",
    "    - Uses tok.model_max_length if valid, else cap_default.\n",
    "    - Leaves a small headroom for special tokens/prefixes.\n",
    "    \"\"\"\n",
    "    max_len = getattr(tok, \"model_max_length\", None)\n",
    "    if max_len is None or max_len > 100_000_000:  # some tokenizers set a huge sentinel\n",
    "        max_len = cap_default\n",
    "    return max(32, int(max_len - headroom))\n",
    "\n",
    "TOKEN_BUDGET = model_token_budget(tokenizer, headroom=16, cap_default=tokenizer.model_max_length)\n",
    "TOKEN_BUDGET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f487060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes you already defined `tokenizer`, TOKEN_BUDGET, and count_tokens() previously\n",
    "\n",
    "def batch_count_tokens(texts: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Much faster than looping tokenizer.encode for each string.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    enc = tokenizer(texts, add_special_tokens=False, padding=False, truncation=False, return_length=True)\n",
    "    # HF returns `length` for each item\n",
    "    return [int(x) for x in enc[\"length\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee997cc6",
   "metadata": {},
   "source": [
    "2) Token helpers (accurate counting + token-level splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf423fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82385f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_tokens(text: str, max_tokens: int, overlap_tokens: int = 0):\n",
    "    \"\"\"\n",
    "    Split text strictly by tokenizer tokens so each segment <= max_tokens.\n",
    "    Overlap is in *tokens*, not characters, and works with any language (CJK too).\n",
    "    \"\"\"\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    out = []\n",
    "    i, n = 0, len(ids)\n",
    "    while i < n:\n",
    "        j = min(i + max_tokens, n)\n",
    "        seg_ids = ids[i:j]\n",
    "        out.append(tokenizer.decode(seg_ids, skip_special_tokens=True).strip())\n",
    "        if j == n:\n",
    "            break\n",
    "        # move start forward, keeping overlap tokens\n",
    "        i = j - overlap_tokens if overlap_tokens and (j - overlap_tokens) > i else j\n",
    "    return [s for s in out if s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_sentences_to_passages_fast(\n",
    "    sentences: List[str],\n",
    "    sent_token_lens: List[int],\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    ") -> List[Tuple[str, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Returns list of (passage_text, start_sent_idx, end_sent_idx_inclusive, chunk_tokens).\n",
    "    Assumes sent_token_lens[i] == token length of sentences[i] (already computed in batch).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    i, n = 0, len(sentences)\n",
    "    while i < n:\n",
    "        cur_sents, cur_tok = [], 0\n",
    "        start_i = i\n",
    "\n",
    "        while i < n:\n",
    "            s_tok = sent_token_lens[i]\n",
    "            if s_tok > token_limit:\n",
    "                # Split the single oversized sentence strictly by tokens (rare)\n",
    "                parts = split_by_tokens(sentences[i], max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, i, i, count_tokens(part)))\n",
    "                i += 1\n",
    "                start_i = i\n",
    "                break\n",
    "\n",
    "            if cur_tok + s_tok <= token_limit:\n",
    "                cur_sents.append(sentences[i])\n",
    "                cur_tok += s_tok\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if cur_sents:\n",
    "            text = \" \".join(cur_sents).strip()\n",
    "            # Safety: enforce token budget in case join changed tokenization\n",
    "            tlen = count_tokens(text)\n",
    "            if tlen > token_limit:\n",
    "                parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, start_i, i - 1, count_tokens(part)))\n",
    "            else:\n",
    "                passages.append((text, start_i, i - 1, tlen))\n",
    "\n",
    "        # prepare overlap by sentences (approximate by token sum)\n",
    "        if i < n and cur_sents:\n",
    "            ov_tok = 0\n",
    "            back = len(cur_sents) - 1\n",
    "            while back >= 0 and ov_tok + sent_token_lens[start_i + back] <= overlap_tokens:\n",
    "                ov_tok += sent_token_lens[start_i + back]\n",
    "                back -= 1\n",
    "            overlap_sents = (len(cur_sents) - 1) - back\n",
    "            if overlap_sents > 0:\n",
    "                i = max(i - overlap_sents, start_i)\n",
    "\n",
    "    # merge tiny tail if possible\n",
    "    if len(passages) >= 2:\n",
    "        last_text, ls, le, ltok = passages[-1]\n",
    "        if ltok < min_chunk_tokens:\n",
    "            prev_text, ps, pe, ptok = passages[-2]\n",
    "            merged = (prev_text + \" \" + last_text).strip()\n",
    "            mlen = count_tokens(merged)\n",
    "            if mlen <= token_limit:\n",
    "                passages[-2] = (merged, ps, le, mlen)\n",
    "                passages.pop()\n",
    "\n",
    "    # Final enforce (paranoid mode)\n",
    "    safe = []\n",
    "    for text, s, e, tlen in passages:\n",
    "        if tlen <= token_limit:\n",
    "            safe.append((text, s, e, tlen))\n",
    "        else:\n",
    "            parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "            for p in parts:\n",
    "                safe.append((p, s, e, count_tokens(p)))\n",
    "    return safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d81e7c",
   "metadata": {},
   "source": [
    "#### 3) Sentence fallback (if your df lacks sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97794ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_sentences(row) -> List[str]:\n",
    "    if isinstance(row.get(\"sentences\"), list) and row[\"sentences\"]:\n",
    "        return row[\"sentences\"]\n",
    "    # fallback: lightweight multilingual splitter\n",
    "    if text_to_sentences is not None:\n",
    "        return [s.strip() for s in text_to_sentences(row.get(\"clean_text\",\"\")).split(\"\\n\") if s.strip()]\n",
    "    # last-resort: split on punctuation (rough)\n",
    "    import re\n",
    "    txt = (row.get(\"clean_text\") or \"\").strip()\n",
    "    parts = re.split(r'(?<=[.!?。\\u3002！？])\\s+', txt)\n",
    "    return [p for p in parts if p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886df029",
   "metadata": {},
   "source": [
    "#### 4) Sentence-wise greedy packing into passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20990a91",
   "metadata": {},
   "source": [
    "- Pack whole sentences until the token_limit would be exceeded.\n",
    "- Keep a small sentence overlap (by tokens) to avoid cutting context.\n",
    "- If a single sentence is longer than the limit, we split that sentence on whitespace to fit.\n",
    "- Ensure the final chunk isn’t tiny (merge forward/back when possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46cfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_sentences_to_passages(\n",
    "    sentences,\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns list of (passage_text, start_sent_idx, end_sent_idx_inclusive),\n",
    "    guaranteeing each passage <= token_limit (tokenizer-accurate).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    i, n = 0, len(sentences)\n",
    "\n",
    "    while i < n:\n",
    "        cur_sents, cur_tok = [], 0\n",
    "        start_i = i\n",
    "\n",
    "        while i < n:\n",
    "            s = sentences[i]\n",
    "            s_tok = count_tokens(s)\n",
    "\n",
    "            # If a single sentence is too long, split it by tokens now.\n",
    "            if s_tok > token_limit:\n",
    "                long_parts = split_by_tokens(s, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in long_parts:\n",
    "                    passages.append((part, i, i))\n",
    "                i += 1\n",
    "                start_i = i  # reset packing window after forcing splits\n",
    "                break\n",
    "\n",
    "            # Try to add sentence; if it would exceed, emit current chunk\n",
    "            if cur_tok + s_tok <= token_limit:\n",
    "                cur_sents.append(s)\n",
    "                cur_tok += s_tok\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if cur_sents:\n",
    "            chunk_text = \" \".join(cur_sents).strip()\n",
    "            # Safety: if the joined chunk still exceeds (rare), split by tokens\n",
    "            if count_tokens(chunk_text) > token_limit:\n",
    "                parts = split_by_tokens(chunk_text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, start_i, i - 1))\n",
    "            else:\n",
    "                passages.append((chunk_text, start_i, i - 1))\n",
    "\n",
    "        # Prepare sentence-level overlap for next window\n",
    "        if i < n and passages and cur_sents:\n",
    "            # choose last sentences whose total tokens ≈ overlap_tokens\n",
    "            ov_sents, ov_tok = [], 0\n",
    "            for s in reversed(cur_sents):\n",
    "                t = count_tokens(s)\n",
    "                if ov_tok + t > overlap_tokens and ov_sents:\n",
    "                    break\n",
    "                ov_sents.insert(0, s)\n",
    "                ov_tok += t\n",
    "            if ov_sents:\n",
    "                i = max(i - len(ov_sents), start_i)\n",
    "\n",
    "    # Merge a too-small tail into the previous chunk (if it keeps budget)\n",
    "    if len(passages) >= 2:\n",
    "        last_text, ls, le = passages[-1]\n",
    "        if count_tokens(last_text) < min_chunk_tokens:\n",
    "            prev_text, ps, pe = passages[-2]\n",
    "            merged = (prev_text + \" \" + last_text).strip()\n",
    "            if count_tokens(merged) <= token_limit:\n",
    "                passages[-2] = (merged, ps, le)\n",
    "                passages.pop()\n",
    "\n",
    "    # Final safety: enforce budget on every chunk (handles corner cases)\n",
    "    safe = []\n",
    "    for text, s, e in passages:\n",
    "        if count_tokens(text) <= token_limit:\n",
    "            safe.append((text, s, e))\n",
    "        else:\n",
    "            parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "            for p in parts:\n",
    "                safe.append((p, s, e))\n",
    "    return safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdacad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sentence(sent: str, token_limit: int) -> List[str]:\n",
    "    \"\"\"If one sentence exceeds token_limit, split it on whitespace to fit.\"\"\"\n",
    "    words = sent.split()\n",
    "    chunks, cur, cur_tok = [], [], 0\n",
    "    for w in words:\n",
    "        t = count_tokens(w + (\" \" if cur else \"\"))\n",
    "        if cur_tok + t > token_limit and cur:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_tok = [], 0\n",
    "        cur.append(w)\n",
    "        cur_tok += t\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [sent]  # fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8526d",
   "metadata": {},
   "source": [
    "#### 5) Apply to your preprocessed DataFrame (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596ee3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_to_passages(\n",
    "    df: pd.DataFrame,\n",
    "    token_limit: int = 350,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    "    batch_size: int = 5000\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total = len(df)\n",
    "    for start in tqdm(range(0, total, batch_size), desc=\"Chunking to passages\"):\n",
    "        part = df.iloc[start:start+batch_size]\n",
    "        for doc_id, row in part.iterrows():\n",
    "            sents = ensure_sentences(row)\n",
    "            chunks = pack_sentences_to_passages(\n",
    "                sents,\n",
    "                token_limit=token_limit,\n",
    "                overlap_tokens=overlap_tokens,\n",
    "                min_chunk_tokens=min_chunk_tokens\n",
    "            )\n",
    "            for chunk_idx, (text, s_start, s_end) in enumerate(chunks):\n",
    "                rows.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_idx,\n",
    "                    \"_site\": row.get(\"_site\", None),\n",
    "                    \"title\": row.get(\"title\", None),\n",
    "                    \"lang\": row.get(\"lang\", None),\n",
    "                    \"chunk_text\": text,\n",
    "                    \"chunk_tokens\": count_tokens(text),\n",
    "                    \"sent_start\": int(s_start),\n",
    "                    \"sent_end\": int(s_end),\n",
    "                    # (optional) keep a small snippet for quick previews\n",
    "                    \"preview\": text[:160].replace(\"\\n\",\" \") + (\"…\" if len(text) > 160 else \"\")\n",
    "                })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78aeb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your target within the model budget (e.g., ~350) but cap at TOKEN_BUDGET\n",
    "TARGET_SIZE = 350\n",
    "EFFECTIVE_LIMIT = min(TARGET_SIZE, TOKEN_BUDGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15427cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def explode_to_passages_fast_batched(\n",
    "    df: pd.DataFrame,\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    "    batch_size: int = 5000,\n",
    "    out_dir: str = \"data/passages_parts\",\n",
    "    out_prefix: str = \"passages_part\",\n",
    "    resume: bool = True,  # skip parts that already exist\n",
    ") -> pd.DataFrame:\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    total_docs = len(df)\n",
    "    part_files = []\n",
    "\n",
    "    pbar = tqdm(range(0, total_docs, batch_size), desc=\"Chunking batches\", unit=\"docs\")\n",
    "    part_idx = 0\n",
    "    for start in pbar:\n",
    "        end = min(start + batch_size, total_docs)\n",
    "        part = df.iloc[start:end].copy()\n",
    "        part_file = os.path.join(out_dir, f\"{out_prefix}_{part_idx:04d}.parquet\")\n",
    "        if resume and os.path.exists(part_file):\n",
    "            part_files.append(part_file)\n",
    "            part_idx += 1\n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        out_rows = []\n",
    "\n",
    "        # Precompute sentence token lengths per doc with **batch tokenization**\n",
    "        for doc_id, row in part.iterrows():\n",
    "            sents = row[\"sentences\"] if isinstance(row.get(\"sentences\"), list) else []\n",
    "            if not sents:\n",
    "                # fallback from clean_text if needed\n",
    "                sents = [row.get(\"clean_text\",\"\")] if row.get(\"clean_text\") else []\n",
    "\n",
    "            sent_lens = batch_count_tokens(sents) if sents else [0]\n",
    "\n",
    "            chunks = pack_sentences_to_passages_fast(\n",
    "                sentences=sents,\n",
    "                sent_token_lens=sent_lens,\n",
    "                token_limit=token_limit,\n",
    "                overlap_tokens=overlap_tokens,\n",
    "                min_chunk_tokens=min_chunk_tokens,\n",
    "            )\n",
    "\n",
    "            for chunk_idx, (text, s_start, s_end, tlen) in enumerate(chunks):\n",
    "                out_rows.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_idx,\n",
    "                    \"site\": row.get(\"site\"),\n",
    "                    \"title\": row.get(\"title\"),\n",
    "                    \"lang\": row.get(\"lang\"),\n",
    "                    \"chunk_text\": text,\n",
    "                    \"chunk_tokens\": tlen,\n",
    "                    \"sent_start\": int(s_start),\n",
    "                    \"sent_end\": int(s_end),\n",
    "                    \"preview\": (text[:160].replace(\"\\n\",\" \") + (\"…\" if len(text) > 160 else \"\")),\n",
    "                })\n",
    "\n",
    "        df_part = pd.DataFrame(out_rows)\n",
    "        df_part.to_parquet(part_file, index=False, compression=\"zstd\")\n",
    "        part_files.append(part_file)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        pbar.set_postfix({\n",
    "            \"docs\": f\"{end}/{total_docs}\",\n",
    "            \"chunks\": len(df_part),\n",
    "            \"sec/batch\": f\"{dt:.1f}\",\n",
    "            \"chunks/s\": f\"{(len(df_part)/(dt+1e-9)):.1f}\",\n",
    "        })\n",
    "        part_idx += 1\n",
    "\n",
    "    # Combine parts (optional; or keep parts for sharded indexing)\n",
    "    df_all = pd.concat((pd.read_parquet(f) for f in part_files), ignore_index=True)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412273b3",
   "metadata": {},
   "source": [
    "### <b>Run tokenization process on pre-processes data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f7579",
   "metadata": {},
   "source": [
    "#### Load preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3272b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load the duplicates file\n",
    "df_processed = pd.read_parquet(\"../shared-data-library/out/df_processed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95477012",
   "metadata": {},
   "source": [
    "#### Validate preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cd97a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>&lt;p&gt;“I have never thought that I can do importa...</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[\"I have never thought that I can do important...</td>\n",
       "      <td>[\", I, have, never, thought, that, I, can, do,...</td>\n",
       "      <td>9</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>&lt;p&gt;We spoke to Heghine for a long time and she...</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>en</td>\n",
       "      <td>[We spoke to Heghine for a long time and she o...</td>\n",
       "      <td>[We, spoke, to, Heghine, for, a, long, time, a...</td>\n",
       "      <td>13</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>&lt;p&gt;Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի...</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "      <td>hy</td>\n",
       "      <td>[Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի V...</td>\n",
       "      <td>[Տավուշի, մարզի, Ներքին, Ծաղկավան, գյուղի, դպր...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>&lt;p&gt;Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղու...</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "      <td>hy</td>\n",
       "      <td>[Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությ...</td>\n",
       "      <td>[Վերջին, երեք, տարիներին, ՅՈՒՆԻՍԵՖ, -, ն, այս,...</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>&lt;p&gt;ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմաս...</td>\n",
       "      <td>ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմասյան...</td>\n",
       "      <td>hy</td>\n",
       "      <td>[ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմասյա...</td>\n",
       "      <td>[ՅՈՒՆԻՍԵՖ, -, ի, ԱՌՆ, ծրագրի, ղեկավար, Տիգրան,...</td>\n",
       "      <td>2</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           site  \\\n",
       "0  armenia__textcontent_article   \n",
       "1  armenia__textcontent_article   \n",
       "2  armenia__textcontent_article   \n",
       "3  armenia__textcontent_article   \n",
       "4  armenia__textcontent_article   \n",
       "\n",
       "                                           title  \\\n",
       "0                           Երբեք չէի պատկերացնի   \n",
       "1                           Երբեք չէի պատկերացնի   \n",
       "2  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   \n",
       "3  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   \n",
       "4  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   \n",
       "\n",
       "                                                text  \\\n",
       "0  <p>“I have never thought that I can do importa...   \n",
       "1  <p>We spoke to Heghine for a long time and she...   \n",
       "2  <p>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի...   \n",
       "3  <p>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղու...   \n",
       "4  <p>ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմաս...   \n",
       "\n",
       "                                          clean_text lang  \\\n",
       "0  \"I have never thought that I can do important ...   en   \n",
       "1  We spoke to Heghine for a long time and she of...   en   \n",
       "2  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...   hy   \n",
       "3  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...   hy   \n",
       "4  ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմասյան...   hy   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [\"I have never thought that I can do important...   \n",
       "1  [We spoke to Heghine for a long time and she o...   \n",
       "2  [Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի V...   \n",
       "3  [Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությ...   \n",
       "4  [ՅՈՒՆԻՍԵՖ-ի ԱՌՆ ծրագրի ղեկավար Տիգրան Թովմասյա...   \n",
       "\n",
       "                                              tokens  n_sentences  n_tokens  \n",
       "0  [\", I, have, never, thought, that, I, can, do,...            9       260  \n",
       "1  [We, spoke, to, Heghine, for, a, long, time, a...           13       357  \n",
       "2  [Տավուշի, մարզի, Ներքին, Ծաղկավան, գյուղի, դպր...            1        96  \n",
       "3  [Վերջին, երեք, տարիներին, ՅՈՒՆԻՍԵՖ, -, ն, այս,...            1       109  \n",
       "4  [ՅՈՒՆԻՍԵՖ, -, ի, ԱՌՆ, ծրագրի, ղեկավար, Տիգրան,...            2       270  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af2d6109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f40a00eb0141be907130a221c4ddce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking batches:   0%|          | 0/5 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Run main function to tokenize processed documents.\n",
    "df_passages = explode_to_passages_fast_batched(\n",
    "    df=df_processed,\n",
    "    token_limit=EFFECTIVE_LIMIT,\n",
    "    overlap_tokens=40,\n",
    "    min_chunk_tokens=120,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    out_dir=\"../shared-data-library/passages\",\n",
    "    out_prefix=\"passages_mE5_350\",\n",
    "    resume=True,   # reruns will skip completed parts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "867f088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 51,968 passages from 21,845 docs.\n",
      "Max tokens: 350 | Budget: 496\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Created {len(df_passages):,} passages from {len(df_processed):,} docs.\")\n",
    "print(\"Max tokens:\", df_passages['chunk_tokens'].max(), \"| Budget:\", TOKEN_BUDGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ff0b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>chunk_tokens</th>\n",
       "      <th>sent_start</th>\n",
       "      <th>sent_end</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>responsibility, this is her opportunity to als...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>responsibility, this is her opportunity to als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>hy</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>hy</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  chunk_id                          site  \\\n",
       "0       0         0  armenia__textcontent_article   \n",
       "1       1         0  armenia__textcontent_article   \n",
       "2       1         1  armenia__textcontent_article   \n",
       "3       2         0  armenia__textcontent_article   \n",
       "4       3         0  armenia__textcontent_article   \n",
       "\n",
       "                                           title lang  \\\n",
       "0                           Երբեք չէի պատկերացնի   en   \n",
       "1                           Երբեք չէի պատկերացնի   en   \n",
       "2                           Երբեք չէի պատկերացնի   en   \n",
       "3  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   hy   \n",
       "4  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   hy   \n",
       "\n",
       "                                          chunk_text  chunk_tokens  \\\n",
       "0  \"I have never thought that I can do important ...           288   \n",
       "1  We spoke to Heghine for a long time and she of...           350   \n",
       "2  responsibility, this is her opportunity to als...            68   \n",
       "3  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...           174   \n",
       "4  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...           181   \n",
       "\n",
       "   sent_start  sent_end                                            preview  \n",
       "0           0         0  \"I have never thought that I can do important ...  \n",
       "1           0         0  We spoke to Heghine for a long time and she of...  \n",
       "2           0         0  responsibility, this is her opportunity to als...  \n",
       "3           0         0  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...  \n",
       "4           0         0  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_passages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed1102",
   "metadata": {},
   "source": [
    "#### 6) Save passages (for embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6236ca5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../shared-data-library/out/df_passages.parquet'), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_passages.to_parquet(OUT_PARQ, index=False, compression=\"zstd\")\n",
    "\n",
    "if OUT_JSONL:\n",
    "    OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in df_passages.to_dict(orient=\"records\"):\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "OUT_PARQ, OUT_JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbfd3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ Notebook executed in 6.54 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"⏱ Notebook executed in %.2f minutes\" % ((time.time()-nb_start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
