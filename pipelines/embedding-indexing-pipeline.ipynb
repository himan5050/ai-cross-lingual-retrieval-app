{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fcbb49",
   "metadata": {},
   "source": [
    "# <b>Embedding and Indexing Pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b48e9b",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d64e10",
   "metadata": {},
   "source": [
    "## <b>Data preparation and text preprocessing pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02b5a7",
   "metadata": {},
   "source": [
    "#### 1) Setup (run once per notebook/kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see \"tokenizers parallelism\" warnings from other libs, silence them:\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36c103",
   "metadata": {},
   "source": [
    "#### 2) Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483170fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import ftfy\n",
    "import langid\n",
    "from blingfire import text_to_sentences, text_to_words\n",
    "\n",
    "# Optional tokenizers (used if available)\n",
    "try:\n",
    "    import jieba  # zh\n",
    "except Exception:\n",
    "    jieba = None\n",
    "\n",
    "try:\n",
    "    from fugashi import Tagger  # ja\n",
    "    _ja_tagger = Tagger()\n",
    "except Exception:\n",
    "    _ja_tagger = None\n",
    "\n",
    "try:\n",
    "    import pythainlp.tokenize as thai_tok  # th\n",
    "except Exception:\n",
    "    thai_tok = None\n",
    "\n",
    "# ---- Configure your paths here ----\n",
    "IN_DIR    = Path(\"datasets/mdocuments\")                 # folder containing many *.csv like site_a.csv, site_b.csv\n",
    "OUT_PARQ  = Path(\"data/multilingual_processed.parquet\")\n",
    "OUT_JSONL = None  # e.g., Path(\"data/multilingual_processed.jsonl\") if you also want JSONL\n",
    "BATCH_SIZE = 5000  # 2k–10k works well; adjust to memory/CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0148d0",
   "metadata": {},
   "source": [
    "#### 3) Cleaning & tokenization helpers (Unicode-safe, multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aba12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compiled regexes\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "CTRL_RE = re.compile(r'[\\u0000-\\u001f\\u007f\\u200b\\u200c\\u200d]')  # control + zero-width chars\n",
    "MULTISPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Unicode-safe normalization & light cleaning that respects multilingual scripts.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    s = ftfy.fix_text(text)                           # fix mojibake\n",
    "    s = unicodedata.normalize(\"NFC\", s)               # compose accents consistently\n",
    "    s = BeautifulSoup(s, \"html.parser\").get_text(\" \") # strip HTML safely\n",
    "    s = URL_RE.sub(\" <URL> \", s)                      # keep a URL placeholder (useful for boundaries)\n",
    "    s = CTRL_RE.sub(\" \", s)                           # drop control / zero-width chars\n",
    "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
    "    # Intentionally NOT lowercasing (Turkish I/ı, German ß, proper nouns)\n",
    "    return s\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"unk\"\n",
    "    code, _ = langid.classify(text)\n",
    "    return code\n",
    "\n",
    "def sent_tokenize(text: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    sents = text_to_sentences(text).split(\"\\n\")\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def word_tokenize(text: str, lang: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    lang = (lang or \"\").split(\"_\")[0]\n",
    "    if lang == \"zh\" and jieba is not None:\n",
    "        return [t.strip() for t in jieba.cut(text) if t.strip()]\n",
    "    if lang == \"ja\" and _ja_tagger is not None:\n",
    "        return [w.surface for w in _ja_tagger(text) if w.surface.strip()]\n",
    "    if lang == \"th\" and thai_tok is not None:\n",
    "        return [t.strip() for t in thai_tok.word_tokenize(text) if t.strip()]\n",
    "    # Default fast multilingual fallback\n",
    "    return [w for w in text_to_words(text).split() if w]\n",
    "\n",
    "def preprocess_row(row: Dict) -> Dict:\n",
    "    raw = row.get(\"text\", \"\")\n",
    "    clean = normalize_text(raw)\n",
    "    lang = detect_lang(clean) if clean else \"unk\"\n",
    "    sents = sent_tokenize(clean)\n",
    "    tokens = word_tokenize(clean, lang)\n",
    "    return {\n",
    "        \"site\": row.get(\"_site\", \"\"),\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"text\": raw,\n",
    "        \"clean_text\": clean,\n",
    "        \"lang\": lang,\n",
    "        \"sentences\": sents,\n",
    "        \"tokens\": tokens,\n",
    "        \"n_sentences\": len(sents),\n",
    "        \"n_tokens\": len(tokens),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea3a64",
   "metadata": {},
   "source": [
    "#### 4) Load many CSVs (keep only title, text, add site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_many_csvs(in_dir: Path) -> pd.DataFrame:\n",
    "    paths = sorted(in_dir.glob(\"*.csv\"))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {in_dir.resolve()}\")\n",
    "\n",
    "    frames = []\n",
    "    for p in tqdm(paths, desc=\"Loading CSVs\"):\n",
    "        site = p.stem  # from \"site_name.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                p,\n",
    "                encoding=\"utf-8-sig\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                dtype={\"title\": \"string\", \"text\": \"string\"},\n",
    "                usecols=lambda c: c in (\"title\", \"text\"),\n",
    "            )\n",
    "        except Exception:\n",
    "            # Fallback if columns/encodings are messy\n",
    "            df = pd.read_csv(p, encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            for col in (\"title\", \"text\"):\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            df = df[[\"title\", \"text\"]]\n",
    "\n",
    "        df[\"title\"] = df[\"title\"].astype(\"string\")\n",
    "        df[\"text\"]  = df[\"text\"].astype(\"string\")\n",
    "        df[\"_site\"] = site\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True)\n",
    "    # Drop rows where both title and text are empty\n",
    "    mask_empty = all_df[\"title\"].fillna(\"\").str.strip().eq(\"\") & all_df[\"text\"].fillna(\"\").str.strip().eq(\"\")\n",
    "    all_df = all_df[~mask_empty].reset_index(drop=True)\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d888a9",
   "metadata": {},
   "source": [
    "#### 5) Remove duplicate texts (Language-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cad3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_dedupe_key(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a canonical key for deduplication:\n",
    "    - normalize with normalize_text (fix mojibake, NFC, strip HTML, keep <URL>, remove control chars)\n",
    "    - collapse whitespace\n",
    "    - casefold for robust, language-aware case-insensitivity\n",
    "    \"\"\"\n",
    "    s = normalize_text(s if isinstance(s, str) else \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s.casefold()\n",
    "\n",
    "def dedupe_by_text(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    # Build key on the raw text (so we can drop before heavy processing)\n",
    "    df[\"__dedupe_key\"] = df[\"text\"].fillna(\"\").map(make_dedupe_key)\n",
    "\n",
    "    # Mark dupes; keep the first occurrence\n",
    "    dup_mask = df[\"__dedupe_key\"].duplicated(keep=\"first\")\n",
    "    n_dups = int(dup_mask.sum())\n",
    "\n",
    "    df_dups   = df.loc[dup_mask, [\"_site\", \"title\", \"text\", \"__dedupe_key\"]].reset_index(drop=True)\n",
    "    df_nodup  = df.loc[~dup_mask].drop(columns=[\"__dedupe_key\"]).reset_index(drop=True)\n",
    "\n",
    "    return df_nodup, df_dups, n_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baa4a5",
   "metadata": {},
   "source": [
    "#### 6) Batch processing (single process—safe in notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8af42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_batched(df: pd.DataFrame, batch_size: int = 5000) -> pd.DataFrame:\n",
    "    df = df[[\"_site\", \"title\", \"text\"]].copy()\n",
    "    n = len(df)\n",
    "    results = []\n",
    "\n",
    "    for start in tqdm(range(0, n, batch_size), desc=\"Processing batches\"):\n",
    "        end = min(n, start + batch_size)\n",
    "        records = df.iloc[start:end].to_dict(orient=\"records\")\n",
    "        out = [preprocess_row(r) for r in records]\n",
    "        part = pd.DataFrame(out)\n",
    "        part[\"lang\"] = part[\"lang\"].astype(\"category\")\n",
    "        results.append(part)\n",
    "\n",
    "    processed = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # If memory becomes an issue, consider these toggles:\n",
    "    # processed[\"tokens\"] = processed[\"tokens\"].apply(lambda t: \" \".join(t))  # store tokens as a single string\n",
    "    # processed = processed.drop(columns=[\"sentences\"])                        # drop sentence list\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba069ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = load_many_csvs(IN_DIR)\n",
    "print(f\"Loaded {len(df_raw):,} rows from {IN_DIR}\")\n",
    "df_raw.head(3)\n",
    "\n",
    "df_nodup, df_dups, n_dups = dedupe_by_text(df_raw)\n",
    "print(f\"🔁 Removed {n_dups:,} duplicate texts. Remaining: {len(df_nodup):,} rows.\")\n",
    "\n",
    "# (Optional) Save a log of removed duplicates for auditing\n",
    "df_dups.to_parquet(\"data/duplicates_removed.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = process_dataframe_batched(df_nodup, batch_size=BATCH_SIZE)\n",
    "print(f\"Processed rows: {len(df_processed):,}\")\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f884c4",
   "metadata": {},
   "source": [
    "#### 7) Save outputs (Parquet is best for Unicode + lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f046e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_processed.to_parquet(OUT_PARQ, index=False, compression=\"zstd\")\n",
    "\n",
    "if OUT_JSONL:\n",
    "    OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in df_processed.to_dict(orient=\"records\"):\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "OUT_PARQ, OUT_JSONL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1246f0",
   "metadata": {},
   "source": [
    "### Review duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd558923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load the duplicates file\n",
    "df_dups = pd.read_parquet(\"data/duplicates_removed.parquet\")\n",
    "\n",
    "# Or load the main processed file\n",
    "df_processed = pd.read_parquet(\"data/multilingual_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ccf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(df_dups):,} duplicates removed\")\n",
    "\n",
    "# Peek at first few duplicates\n",
    "df_dups.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group duplicates to see clusters of texts that were considered the same\n",
    "dup_groups = df_dups.groupby(\"__dedupe_key\")[\"text\"].apply(list).reset_index()\n",
    "dup_groups.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dedupe_key(s: str) -> str:\n",
    "    import re, unicodedata, ftfy\n",
    "    from bs4 import BeautifulSoup\n",
    "    s = ftfy.fix_text(s if isinstance(s, str) else \"\")\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = BeautifulSoup(s, \"html.parser\").get_text(\" \")\n",
    "    s = re.sub(r'https?://\\S+|www\\.\\S+', \" <URL> \", s)\n",
    "    s = re.sub(r'[\\u0000-\\u001f\\u007f\\u200b\\u200c\\u200d]', \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s.casefold()\n",
    "\n",
    "df_dups[\"__dedupe_key\"] = df_dups[\"text\"].fillna(\"\").map(make_dedupe_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509b126",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ff2ba",
   "metadata": {},
   "source": [
    "## <b>Document chunking and embedding pipeline.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab3d16",
   "metadata": {},
   "source": [
    "#### 1) Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q transformers  # for accurate token counting with your HF model\n",
    "# If you didn't use my earlier cells:\n",
    "# %pip install -q blingfire ftfy beautifulsoup4 langid tqdm pyarrow\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# If you need sentence splitting fallback:\n",
    "try:\n",
    "    from blingfire import text_to_sentences\n",
    "except Exception:\n",
    "    text_to_sentences = None\n",
    "\n",
    "# Hugging Face tokenizer for accurate token counts (choose your embedding model)\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d34d40",
   "metadata": {},
   "source": [
    "#### 2) Choose a tokenizer (match your embedding model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418c9e5",
   "metadata": {},
   "source": [
    "Pick the tokenizer that matches the embedding model you’ll use in retrieval (so chunk sizes reflect the true token budget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenizer that matches your embedding model:\n",
    "# Examples:\n",
    "# TOKENIZER_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "# TOKENIZER_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TOKENIZER_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "\n",
    "def model_token_budget(tok, headroom=16, cap_default=512):\n",
    "    \"\"\"\n",
    "    Returns a safe per-chunk token budget:\n",
    "    - Uses tok.model_max_length if valid, else cap_default.\n",
    "    - Leaves a small headroom for special tokens/prefixes.\n",
    "    \"\"\"\n",
    "    max_len = getattr(tok, \"model_max_length\", None)\n",
    "    if max_len is None or max_len > 100_000_000:  # some tokenizers set a huge sentinel\n",
    "        max_len = cap_default\n",
    "    return max(32, int(max_len - headroom))\n",
    "\n",
    "TOKEN_BUDGET = model_token_budget(tokenizer, headroom=16, cap_default=512)\n",
    "TOKEN_BUDGET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any\n",
    "import math, time, os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Assumes you already defined `tokenizer`, TOKEN_BUDGET, and count_tokens() previously\n",
    "\n",
    "def batch_count_tokens(texts: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Much faster than looping tokenizer.encode for each string.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    enc = tokenizer(texts, add_special_tokens=False, padding=False, truncation=False, return_length=True)\n",
    "    # HF returns `length` for each item\n",
    "    return [int(x) for x in enc[\"length\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8202a71",
   "metadata": {},
   "source": [
    "2) Token helpers (accurate counting + token-level splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836afde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def split_by_tokens(text: str, max_tokens: int, overlap_tokens: int = 0):\n",
    "    \"\"\"\n",
    "    Split text strictly by tokenizer tokens so each segment <= max_tokens.\n",
    "    Overlap is in *tokens*, not characters, and works with any language (CJK too).\n",
    "    \"\"\"\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    out = []\n",
    "    i, n = 0, len(ids)\n",
    "    while i < n:\n",
    "        j = min(i + max_tokens, n)\n",
    "        seg_ids = ids[i:j]\n",
    "        out.append(tokenizer.decode(seg_ids, skip_special_tokens=True).strip())\n",
    "        if j == n:\n",
    "            break\n",
    "        # move start forward, keeping overlap tokens\n",
    "        i = j - overlap_tokens if overlap_tokens and (j - overlap_tokens) > i else j\n",
    "    return [s for s in out if s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_sentences_to_passages_fast(\n",
    "    sentences: List[str],\n",
    "    sent_token_lens: List[int],\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    ") -> List[Tuple[str, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Returns list of (passage_text, start_sent_idx, end_sent_idx_inclusive, chunk_tokens).\n",
    "    Assumes sent_token_lens[i] == token length of sentences[i] (already computed in batch).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    i, n = 0, len(sentences)\n",
    "    while i < n:\n",
    "        cur_sents, cur_tok = [], 0\n",
    "        start_i = i\n",
    "\n",
    "        while i < n:\n",
    "            s_tok = sent_token_lens[i]\n",
    "            if s_tok > token_limit:\n",
    "                # Split the single oversized sentence strictly by tokens (rare)\n",
    "                parts = split_by_tokens(sentences[i], max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, i, i, count_tokens(part)))\n",
    "                i += 1\n",
    "                start_i = i\n",
    "                break\n",
    "\n",
    "            if cur_tok + s_tok <= token_limit:\n",
    "                cur_sents.append(sentences[i])\n",
    "                cur_tok += s_tok\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if cur_sents:\n",
    "            text = \" \".join(cur_sents).strip()\n",
    "            # Safety: enforce token budget in case join changed tokenization\n",
    "            tlen = count_tokens(text)\n",
    "            if tlen > token_limit:\n",
    "                parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, start_i, i - 1, count_tokens(part)))\n",
    "            else:\n",
    "                passages.append((text, start_i, i - 1, tlen))\n",
    "\n",
    "        # prepare overlap by sentences (approximate by token sum)\n",
    "        if i < n and cur_sents:\n",
    "            ov_tok = 0\n",
    "            back = len(cur_sents) - 1\n",
    "            while back >= 0 and ov_tok + sent_token_lens[start_i + back] <= overlap_tokens:\n",
    "                ov_tok += sent_token_lens[start_i + back]\n",
    "                back -= 1\n",
    "            overlap_sents = (len(cur_sents) - 1) - back\n",
    "            if overlap_sents > 0:\n",
    "                i = max(i - overlap_sents, start_i)\n",
    "\n",
    "    # merge tiny tail if possible\n",
    "    if len(passages) >= 2:\n",
    "        last_text, ls, le, ltok = passages[-1]\n",
    "        if ltok < min_chunk_tokens:\n",
    "            prev_text, ps, pe, ptok = passages[-2]\n",
    "            merged = (prev_text + \" \" + last_text).strip()\n",
    "            mlen = count_tokens(merged)\n",
    "            if mlen <= token_limit:\n",
    "                passages[-2] = (merged, ps, le, mlen)\n",
    "                passages.pop()\n",
    "\n",
    "    # Final enforce (paranoid mode)\n",
    "    safe = []\n",
    "    for text, s, e, tlen in passages:\n",
    "        if tlen <= token_limit:\n",
    "            safe.append((text, s, e, tlen))\n",
    "        else:\n",
    "            parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "            for p in parts:\n",
    "                safe.append((p, s, e, count_tokens(p)))\n",
    "    return safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c4a85",
   "metadata": {},
   "source": [
    "#### 3) Sentence fallback (if your df lacks sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd486078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_sentences(row) -> List[str]:\n",
    "    if isinstance(row.get(\"sentences\"), list) and row[\"sentences\"]:\n",
    "        return row[\"sentences\"]\n",
    "    # fallback: lightweight multilingual splitter\n",
    "    if text_to_sentences is not None:\n",
    "        return [s.strip() for s in text_to_sentences(row.get(\"clean_text\",\"\")).split(\"\\n\") if s.strip()]\n",
    "    # last-resort: split on punctuation (rough)\n",
    "    import re\n",
    "    txt = (row.get(\"clean_text\") or \"\").strip()\n",
    "    parts = re.split(r'(?<=[.!?。\\u3002！？])\\s+', txt)\n",
    "    return [p for p in parts if p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86fcef",
   "metadata": {},
   "source": [
    "#### 4) Sentence-wise greedy packing into passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f593dcb",
   "metadata": {},
   "source": [
    "- Pack whole sentences until the token_limit would be exceeded.\n",
    "- Keep a small sentence overlap (by tokens) to avoid cutting context.\n",
    "- If a single sentence is longer than the limit, we split that sentence on whitespace to fit.\n",
    "- Ensure the final chunk isn’t tiny (merge forward/back when possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_sentences_to_passages(\n",
    "    sentences,\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns list of (passage_text, start_sent_idx, end_sent_idx_inclusive),\n",
    "    guaranteeing each passage <= token_limit (tokenizer-accurate).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    i, n = 0, len(sentences)\n",
    "\n",
    "    while i < n:\n",
    "        cur_sents, cur_tok = [], 0\n",
    "        start_i = i\n",
    "\n",
    "        while i < n:\n",
    "            s = sentences[i]\n",
    "            s_tok = count_tokens(s)\n",
    "\n",
    "            # If a single sentence is too long, split it by tokens now.\n",
    "            if s_tok > token_limit:\n",
    "                long_parts = split_by_tokens(s, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in long_parts:\n",
    "                    passages.append((part, i, i))\n",
    "                i += 1\n",
    "                start_i = i  # reset packing window after forcing splits\n",
    "                break\n",
    "\n",
    "            # Try to add sentence; if it would exceed, emit current chunk\n",
    "            if cur_tok + s_tok <= token_limit:\n",
    "                cur_sents.append(s)\n",
    "                cur_tok += s_tok\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if cur_sents:\n",
    "            chunk_text = \" \".join(cur_sents).strip()\n",
    "            # Safety: if the joined chunk still exceeds (rare), split by tokens\n",
    "            if count_tokens(chunk_text) > token_limit:\n",
    "                parts = split_by_tokens(chunk_text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "                for part in parts:\n",
    "                    passages.append((part, start_i, i - 1))\n",
    "            else:\n",
    "                passages.append((chunk_text, start_i, i - 1))\n",
    "\n",
    "        # Prepare sentence-level overlap for next window\n",
    "        if i < n and passages and cur_sents:\n",
    "            # choose last sentences whose total tokens ≈ overlap_tokens\n",
    "            ov_sents, ov_tok = [], 0\n",
    "            for s in reversed(cur_sents):\n",
    "                t = count_tokens(s)\n",
    "                if ov_tok + t > overlap_tokens and ov_sents:\n",
    "                    break\n",
    "                ov_sents.insert(0, s)\n",
    "                ov_tok += t\n",
    "            if ov_sents:\n",
    "                i = max(i - len(ov_sents), start_i)\n",
    "\n",
    "    # Merge a too-small tail into the previous chunk (if it keeps budget)\n",
    "    if len(passages) >= 2:\n",
    "        last_text, ls, le = passages[-1]\n",
    "        if count_tokens(last_text) < min_chunk_tokens:\n",
    "            prev_text, ps, pe = passages[-2]\n",
    "            merged = (prev_text + \" \" + last_text).strip()\n",
    "            if count_tokens(merged) <= token_limit:\n",
    "                passages[-2] = (merged, ps, le)\n",
    "                passages.pop()\n",
    "\n",
    "    # Final safety: enforce budget on every chunk (handles corner cases)\n",
    "    safe = []\n",
    "    for text, s, e in passages:\n",
    "        if count_tokens(text) <= token_limit:\n",
    "            safe.append((text, s, e))\n",
    "        else:\n",
    "            parts = split_by_tokens(text, max_tokens=token_limit, overlap_tokens=overlap_tokens // 2)\n",
    "            for p in parts:\n",
    "                safe.append((p, s, e))\n",
    "    return safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sentence(sent: str, token_limit: int) -> List[str]:\n",
    "    \"\"\"If one sentence exceeds token_limit, split it on whitespace to fit.\"\"\"\n",
    "    words = sent.split()\n",
    "    chunks, cur, cur_tok = [], [], 0\n",
    "    for w in words:\n",
    "        t = count_tokens(w + (\" \" if cur else \"\"))\n",
    "        if cur_tok + t > token_limit and cur:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_tok = [], 0\n",
    "        cur.append(w)\n",
    "        cur_tok += t\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [sent]  # fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fca21",
   "metadata": {},
   "source": [
    "#### 5) Apply to your preprocessed DataFrame (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d61dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_to_passages(\n",
    "    df: pd.DataFrame,\n",
    "    token_limit: int = 350,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    "    batch_size: int = 5000\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total = len(df)\n",
    "    for start in tqdm(range(0, total, batch_size), desc=\"Chunking to passages\"):\n",
    "        part = df.iloc[start:start+batch_size]\n",
    "        for doc_id, row in part.iterrows():\n",
    "            sents = ensure_sentences(row)\n",
    "            chunks = pack_sentences_to_passages(\n",
    "                sents,\n",
    "                token_limit=token_limit,\n",
    "                overlap_tokens=overlap_tokens,\n",
    "                min_chunk_tokens=min_chunk_tokens\n",
    "            )\n",
    "            for chunk_idx, (text, s_start, s_end) in enumerate(chunks):\n",
    "                rows.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_idx,\n",
    "                    \"_site\": row.get(\"_site\", None),\n",
    "                    \"title\": row.get(\"title\", None),\n",
    "                    \"lang\": row.get(\"lang\", None),\n",
    "                    \"chunk_text\": text,\n",
    "                    \"chunk_tokens\": count_tokens(text),\n",
    "                    \"sent_start\": int(s_start),\n",
    "                    \"sent_end\": int(s_end),\n",
    "                    # (optional) keep a small snippet for quick previews\n",
    "                    \"preview\": text[:160].replace(\"\\n\",\" \") + (\"…\" if len(text) > 160 else \"\")\n",
    "                })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a26cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your target within the model budget (e.g., ~350) but cap at TOKEN_BUDGET\n",
    "TARGET_SIZE = 350\n",
    "EFFECTIVE_LIMIT = min(TARGET_SIZE, TOKEN_BUDGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFFECTIVE_LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def explode_to_passages_fast_batched(\n",
    "    df: pd.DataFrame,\n",
    "    token_limit: int,\n",
    "    overlap_tokens: int = 40,\n",
    "    min_chunk_tokens: int = 120,\n",
    "    batch_size: int = 5000,\n",
    "    out_dir: str = \"data/passages_parts\",\n",
    "    out_prefix: str = \"passages_part\",\n",
    "    resume: bool = True,  # skip parts that already exist\n",
    ") -> pd.DataFrame:\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    total_docs = len(df)\n",
    "    part_files = []\n",
    "\n",
    "    pbar = tqdm(range(0, total_docs, batch_size), desc=\"Chunking batches\", unit=\"docs\")\n",
    "    part_idx = 0\n",
    "    for start in pbar:\n",
    "        end = min(start + batch_size, total_docs)\n",
    "        part = df.iloc[start:end].copy()\n",
    "        part_file = os.path.join(out_dir, f\"{out_prefix}_{part_idx:04d}.parquet\")\n",
    "        if resume and os.path.exists(part_file):\n",
    "            part_files.append(part_file)\n",
    "            part_idx += 1\n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        out_rows = []\n",
    "\n",
    "        # Precompute sentence token lengths per doc with **batch tokenization**\n",
    "        for doc_id, row in part.iterrows():\n",
    "            sents = row[\"sentences\"] if isinstance(row.get(\"sentences\"), list) else []\n",
    "            if not sents:\n",
    "                # fallback from clean_text if needed\n",
    "                sents = [row.get(\"clean_text\",\"\")] if row.get(\"clean_text\") else []\n",
    "\n",
    "            sent_lens = batch_count_tokens(sents) if sents else [0]\n",
    "\n",
    "            chunks = pack_sentences_to_passages_fast(\n",
    "                sentences=sents,\n",
    "                sent_token_lens=sent_lens,\n",
    "                token_limit=token_limit,\n",
    "                overlap_tokens=overlap_tokens,\n",
    "                min_chunk_tokens=min_chunk_tokens,\n",
    "            )\n",
    "\n",
    "            for chunk_idx, (text, s_start, s_end, tlen) in enumerate(chunks):\n",
    "                out_rows.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_idx,\n",
    "                    \"_site\": row.get(\"_site\"),\n",
    "                    \"title\": row.get(\"title\"),\n",
    "                    \"lang\": row.get(\"lang\"),\n",
    "                    \"chunk_text\": text,\n",
    "                    \"chunk_tokens\": tlen,\n",
    "                    \"sent_start\": int(s_start),\n",
    "                    \"sent_end\": int(s_end),\n",
    "                    \"preview\": (text[:160].replace(\"\\n\",\" \") + (\"…\" if len(text) > 160 else \"\")),\n",
    "                })\n",
    "\n",
    "        df_part = pd.DataFrame(out_rows)\n",
    "        df_part.to_parquet(part_file, index=False, compression=\"zstd\")\n",
    "        part_files.append(part_file)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        pbar.set_postfix({\n",
    "            \"docs\": f\"{end}/{total_docs}\",\n",
    "            \"chunks\": len(df_part),\n",
    "            \"sec/batch\": f\"{dt:.1f}\",\n",
    "            \"chunks/s\": f\"{(len(df_part)/(dt+1e-9)):.1f}\",\n",
    "        })\n",
    "        part_idx += 1\n",
    "\n",
    "    # Combine parts (optional; or keep parts for sharded indexing)\n",
    "    df_all = pd.concat((pd.read_parquet(f) for f in part_files), ignore_index=True)\n",
    "    return df_all\n",
    "\n",
    "# Use your model budget (from previous step)\n",
    "TARGET_SIZE = 350\n",
    "EFFECTIVE_LIMIT = min(TARGET_SIZE, TOKEN_BUDGET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76befc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passages = explode_to_passages_fast_batched(\n",
    "    df=df_processed,\n",
    "    token_limit=EFFECTIVE_LIMIT,\n",
    "    overlap_tokens=40,\n",
    "    min_chunk_tokens=120,\n",
    "    batch_size=5000,\n",
    "    out_dir=\"data/passages_parts\",\n",
    "    out_prefix=\"passages_mE5_350\",\n",
    "    resume=True,   # reruns will skip completed parts\n",
    ")\n",
    "\n",
    "print(f\"✅ Created {len(df_passages):,} passages from {len(df_processed):,} docs.\")\n",
    "print(\"Max tokens:\", df_passages['chunk_tokens'].max(), \"| Budget:\", TOKEN_BUDGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c90a65",
   "metadata": {},
   "source": [
    "#### 6) Save passages (for indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cba2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parq_passages = \"data/passages_mE5_350.parquet\"  # include model/limit in name for clarity\n",
    "pd.Series(df_passages.columns.tolist(), name=\"columns\")  # quick glance\n",
    "df_passages.to_parquet(out_parq_passages, index=False, compression=\"zstd\")\n",
    "out_parq_passages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2bed9",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4da27",
   "metadata": {},
   "source": [
    "## <b>Multilingual embedding pipeline:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d58a0",
   "metadata": {},
   "source": [
    "#### 1) Imports, device, and model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"   # was \"false\" earlier for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "692f88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BetterTransformer is deprecated and will be removed in Optimum v2.0.\n"
     ]
    }
   ],
   "source": [
    "# 0) Config\n",
    "import os, math, time, numpy as np, pandas as pd, torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"true\")  # speed up tokenization\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE  = torch.float16 if DEVICE.type == \"cuda\" else torch.float32\n",
    "\n",
    "# Choose model (speed tiers):\n",
    "# FAST:    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" (384d)\n",
    "# BALANCE: \"intfloat/multilingual-e5-base\" (768d)\n",
    "# QUALITY: \"intfloat/multilingual-e5-large\" (1024d)\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=(torch.float16 if DEVICE.type==\"cuda\" else None))\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# Enable fused attention (BetterTransformer) if available\n",
    "try:\n",
    "    from optimum.bettertransformer import BetterTransformer\n",
    "    model = BetterTransformer.transform(model)\n",
    "except Exception:\n",
    "    pass  # it's fine if not installed\n",
    "\n",
    "def model_token_budget(tok, headroom=16, cap_default=512):\n",
    "    ml = getattr(tok, \"model_max_length\", None)\n",
    "    if ml is None or ml > 100_000_000: ml = cap_default\n",
    "    return max(32, int(ml - headroom))\n",
    "TOKEN_BUDGET = model_token_budget(tokenizer)\n",
    "\n",
    "def add_passage_prefix(texts):\n",
    "    # Only E5 needs \"passage: \" prefix\n",
    "    return [f\"passage: {t}\" for t in texts] if \"intfloat/multilingual-e5\" in MODEL_NAME.lower() else texts\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / counts\n",
    "\n",
    "def embed_batch(texts, max_len=TOKEN_BUDGET):\n",
    "    enc = tokenizer(\n",
    "        texts, padding=True, truncation=True, max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc = {k: v.to(DEVICE, non_blocking=True) for k, v in enc.items()}\n",
    "    with torch.inference_mode(), (\n",
    "        torch.autocast(device_type=DEVICE.type, dtype=torch.float16) if DEVICE.type==\"cuda\" else torch.no_grad()\n",
    "    ):\n",
    "        out = model(**enc)\n",
    "        pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "    # Keep float32 for FAISS stability downstream\n",
    "    return pooled.to(torch.float32).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83bc27c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03fd5be5d874266b642cd5d8dec4046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding on cpu:   0%|          | 0/198 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1696/51968 | 46.1 chunks/s | ETA 1089.5s\n",
      "done 1952/51968 | 24.8 chunks/s | ETA 2016.6s\n",
      "done 2208/51968 | 18.5 chunks/s | ETA 2690.3s\n",
      "done 2464/51968 | 15.4 chunks/s | ETA 3220.0s\n",
      "done 2720/51968 | 13.7 chunks/s | ETA 3586.0s\n",
      "done 2976/51968 | 12.5 chunks/s | ETA 3927.1s\n",
      "done 3232/51968 | 11.5 chunks/s | ETA 4238.4s\n",
      "done 3488/51968 | 10.8 chunks/s | ETA 4487.0s\n",
      "done 3744/51968 | 10.3 chunks/s | ETA 4697.6s\n",
      "done 4000/51968 | 9.6 chunks/s | ETA 4976.9s\n",
      "done 4256/51968 | 9.3 chunks/s | ETA 5118.1s\n",
      "done 4512/51968 | 9.0 chunks/s | ETA 5250.0s\n",
      "done 4768/51968 | 8.8 chunks/s | ETA 5337.4s\n",
      "done 5024/51968 | 8.6 chunks/s | ETA 5430.3s\n",
      "done 5280/51968 | 8.5 chunks/s | ETA 5511.0s\n",
      "done 5536/51968 | 8.3 chunks/s | ETA 5589.1s\n",
      "done 5792/51968 | 8.2 chunks/s | ETA 5634.5s\n",
      "done 6048/51968 | 8.0 chunks/s | ETA 5773.9s\n",
      "done 6304/51968 | 7.9 chunks/s | ETA 5816.1s\n",
      "done 6560/51968 | 7.8 chunks/s | ETA 5850.6s\n",
      "done 6816/51968 | 7.7 chunks/s | ETA 5883.8s\n",
      "done 7072/51968 | 7.6 chunks/s | ETA 5940.6s\n",
      "done 7328/51968 | 7.5 chunks/s | ETA 5954.3s\n",
      "done 7584/51968 | 7.5 chunks/s | ETA 5956.8s\n",
      "done 7840/51968 | 7.4 chunks/s | ETA 5962.1s\n",
      "done 8096/51968 | 7.4 chunks/s | ETA 5964.2s\n",
      "done 8352/51968 | 7.3 chunks/s | ETA 5958.4s\n",
      "done 8608/51968 | 7.3 chunks/s | ETA 5956.8s\n",
      "done 8864/51968 | 7.2 chunks/s | ETA 5953.5s\n",
      "done 9120/51968 | 7.2 chunks/s | ETA 5959.3s\n",
      "done 9376/51968 | 7.2 chunks/s | ETA 5942.9s\n",
      "done 9632/51968 | 7.1 chunks/s | ETA 5927.8s\n",
      "done 9888/51968 | 7.1 chunks/s | ETA 5907.7s\n",
      "done 10144/51968 | 7.0 chunks/s | ETA 5959.4s\n",
      "done 10400/51968 | 7.0 chunks/s | ETA 5964.8s\n",
      "done 10656/51968 | 6.9 chunks/s | ETA 5951.6s\n",
      "done 10912/51968 | 6.9 chunks/s | ETA 5937.8s\n",
      "done 11168/51968 | 6.9 chunks/s | ETA 5920.8s\n",
      "done 11424/51968 | 6.9 chunks/s | ETA 5902.2s\n",
      "done 11680/51968 | 6.8 chunks/s | ETA 5886.2s\n",
      "done 11936/51968 | 6.8 chunks/s | ETA 5868.7s\n",
      "done 12192/51968 | 6.8 chunks/s | ETA 5851.9s\n",
      "done 12448/51968 | 6.8 chunks/s | ETA 5821.9s\n",
      "done 12704/51968 | 6.8 chunks/s | ETA 5793.5s\n",
      "done 12960/51968 | 6.8 chunks/s | ETA 5763.7s\n",
      "done 13216/51968 | 6.8 chunks/s | ETA 5732.0s\n",
      "done 13472/51968 | 6.7 chunks/s | ETA 5706.3s\n",
      "done 13728/51968 | 6.7 chunks/s | ETA 5683.4s\n",
      "done 13984/51968 | 6.7 chunks/s | ETA 5658.9s\n",
      "done 14240/51968 | 6.7 chunks/s | ETA 5638.0s\n",
      "done 14496/51968 | 6.7 chunks/s | ETA 5612.0s\n",
      "done 14752/51968 | 6.6 chunks/s | ETA 5606.3s\n",
      "done 15008/51968 | 6.6 chunks/s | ETA 5580.0s\n",
      "done 15264/51968 | 6.6 chunks/s | ETA 5557.0s\n",
      "done 15520/51968 | 6.6 chunks/s | ETA 5529.7s\n",
      "done 15776/51968 | 6.6 chunks/s | ETA 5507.5s\n",
      "done 16032/51968 | 6.6 chunks/s | ETA 5482.4s\n",
      "done 16288/51968 | 6.5 chunks/s | ETA 5454.1s\n",
      "done 16544/51968 | 6.5 chunks/s | ETA 5421.6s\n",
      "done 16800/51968 | 6.5 chunks/s | ETA 5391.2s\n",
      "done 17056/51968 | 6.5 chunks/s | ETA 5359.3s\n",
      "done 17312/51968 | 6.5 chunks/s | ETA 5326.6s\n",
      "done 17568/51968 | 6.5 chunks/s | ETA 5291.6s\n",
      "done 17824/51968 | 6.5 chunks/s | ETA 5260.1s\n",
      "done 18080/51968 | 6.5 chunks/s | ETA 5227.3s\n",
      "done 18336/51968 | 6.5 chunks/s | ETA 5196.2s\n",
      "done 18592/51968 | 6.5 chunks/s | ETA 5164.3s\n",
      "done 18848/51968 | 6.5 chunks/s | ETA 5134.5s\n",
      "done 19104/51968 | 6.4 chunks/s | ETA 5100.4s\n",
      "done 19360/51968 | 6.4 chunks/s | ETA 5073.6s\n",
      "done 19616/51968 | 6.4 chunks/s | ETA 5045.6s\n",
      "done 19872/51968 | 6.4 chunks/s | ETA 5016.8s\n",
      "done 20128/51968 | 6.4 chunks/s | ETA 4990.0s\n",
      "done 20384/51968 | 6.4 chunks/s | ETA 4958.7s\n",
      "done 20640/51968 | 6.3 chunks/s | ETA 4935.9s\n",
      "done 20896/51968 | 6.3 chunks/s | ETA 4910.5s\n",
      "done 21152/51968 | 6.3 chunks/s | ETA 4883.9s\n",
      "done 21408/51968 | 6.3 chunks/s | ETA 4857.3s\n",
      "done 21664/51968 | 6.3 chunks/s | ETA 4837.3s\n",
      "done 21920/51968 | 6.2 chunks/s | ETA 4817.9s\n",
      "done 22176/51968 | 6.2 chunks/s | ETA 4791.4s\n",
      "done 22432/51968 | 6.2 chunks/s | ETA 4760.6s\n",
      "done 22688/51968 | 6.2 chunks/s | ETA 4732.0s\n",
      "done 22944/51968 | 6.2 chunks/s | ETA 4700.8s\n",
      "done 23200/51968 | 6.2 chunks/s | ETA 4671.4s\n",
      "done 23456/51968 | 6.1 chunks/s | ETA 4639.8s\n",
      "done 23712/51968 | 6.1 chunks/s | ETA 4614.8s\n",
      "done 23968/51968 | 6.1 chunks/s | ETA 4575.7s\n",
      "done 24224/51968 | 6.1 chunks/s | ETA 4536.6s\n",
      "done 24480/51968 | 6.1 chunks/s | ETA 4502.0s\n",
      "done 24736/51968 | 6.1 chunks/s | ETA 4472.1s\n",
      "done 24992/51968 | 6.1 chunks/s | ETA 4442.5s\n",
      "done 25248/51968 | 6.1 chunks/s | ETA 4406.2s\n",
      "done 25504/51968 | 6.1 chunks/s | ETA 4366.7s\n",
      "done 25760/51968 | 6.1 chunks/s | ETA 4325.2s\n",
      "done 26016/51968 | 6.1 chunks/s | ETA 4286.8s\n",
      "done 26272/51968 | 6.1 chunks/s | ETA 4246.0s\n",
      "done 26528/51968 | 6.0 chunks/s | ETA 4205.3s\n",
      "done 26784/51968 | 6.0 chunks/s | ETA 4164.8s\n",
      "done 27040/51968 | 6.0 chunks/s | ETA 4125.7s\n",
      "done 27296/51968 | 6.0 chunks/s | ETA 4085.0s\n",
      "done 27552/51968 | 6.0 chunks/s | ETA 4043.6s\n",
      "done 27808/51968 | 6.0 chunks/s | ETA 4002.1s\n",
      "done 28064/51968 | 6.0 chunks/s | ETA 3963.1s\n",
      "done 28320/51968 | 6.0 chunks/s | ETA 3925.3s\n",
      "done 28576/51968 | 6.0 chunks/s | ETA 3881.7s\n",
      "done 28832/51968 | 6.0 chunks/s | ETA 3839.2s\n",
      "done 29088/51968 | 6.0 chunks/s | ETA 3794.7s\n",
      "done 29344/51968 | 6.0 chunks/s | ETA 3750.8s\n",
      "done 29600/51968 | 6.0 chunks/s | ETA 3707.4s\n",
      "done 29856/51968 | 6.0 chunks/s | ETA 3663.3s\n",
      "done 30112/51968 | 6.0 chunks/s | ETA 3620.8s\n",
      "done 30368/51968 | 6.0 chunks/s | ETA 3577.9s\n",
      "done 30624/51968 | 6.0 chunks/s | ETA 3535.7s\n",
      "done 30880/51968 | 6.0 chunks/s | ETA 3493.4s\n",
      "done 31136/51968 | 6.0 chunks/s | ETA 3449.9s\n",
      "done 31392/51968 | 6.0 chunks/s | ETA 3408.5s\n",
      "done 31648/51968 | 6.0 chunks/s | ETA 3366.9s\n",
      "done 31904/51968 | 6.0 chunks/s | ETA 3324.2s\n",
      "done 32160/51968 | 6.0 chunks/s | ETA 3282.8s\n",
      "done 32416/51968 | 6.0 chunks/s | ETA 3240.7s\n",
      "done 32672/51968 | 6.0 chunks/s | ETA 3198.9s\n",
      "done 32928/51968 | 6.0 chunks/s | ETA 3156.6s\n",
      "done 33184/51968 | 6.0 chunks/s | ETA 3114.7s\n",
      "done 33440/51968 | 6.0 chunks/s | ETA 3073.0s\n",
      "done 33696/51968 | 6.0 chunks/s | ETA 3031.2s\n",
      "done 33952/51968 | 6.0 chunks/s | ETA 2990.1s\n",
      "done 34208/51968 | 6.0 chunks/s | ETA 2948.2s\n",
      "done 34464/51968 | 6.0 chunks/s | ETA 2906.8s\n",
      "done 34720/51968 | 6.0 chunks/s | ETA 2865.2s\n",
      "done 34976/51968 | 6.0 chunks/s | ETA 2824.0s\n",
      "done 35232/51968 | 6.0 chunks/s | ETA 2781.1s\n",
      "done 35488/51968 | 6.0 chunks/s | ETA 2740.7s\n",
      "done 35744/51968 | 6.0 chunks/s | ETA 2699.1s\n",
      "done 36000/51968 | 6.0 chunks/s | ETA 2657.2s\n",
      "done 36256/51968 | 6.0 chunks/s | ETA 2615.1s\n",
      "done 36512/51968 | 6.0 chunks/s | ETA 2573.1s\n",
      "done 36768/51968 | 6.0 chunks/s | ETA 2530.7s\n",
      "done 37024/51968 | 6.0 chunks/s | ETA 2489.7s\n",
      "done 37280/51968 | 6.0 chunks/s | ETA 2448.3s\n",
      "done 37536/51968 | 6.0 chunks/s | ETA 2406.3s\n",
      "done 37792/51968 | 6.0 chunks/s | ETA 2364.4s\n",
      "done 38048/51968 | 6.0 chunks/s | ETA 2323.1s\n",
      "done 38304/51968 | 6.0 chunks/s | ETA 2282.1s\n",
      "done 38560/51968 | 6.0 chunks/s | ETA 2239.5s\n",
      "done 38816/51968 | 6.0 chunks/s | ETA 2198.2s\n",
      "done 39072/51968 | 6.0 chunks/s | ETA 2155.7s\n",
      "done 39328/51968 | 6.0 chunks/s | ETA 2114.0s\n",
      "done 39584/51968 | 6.0 chunks/s | ETA 2071.5s\n",
      "done 39840/51968 | 6.0 chunks/s | ETA 2029.6s\n",
      "done 40096/51968 | 6.0 chunks/s | ETA 1987.4s\n",
      "done 40352/51968 | 6.0 chunks/s | ETA 1945.5s\n",
      "done 40608/51968 | 6.0 chunks/s | ETA 1903.0s\n",
      "done 40864/51968 | 6.0 chunks/s | ETA 1860.8s\n",
      "done 41120/51968 | 6.0 chunks/s | ETA 1819.1s\n",
      "done 41376/51968 | 6.0 chunks/s | ETA 1777.0s\n",
      "done 41632/51968 | 6.0 chunks/s | ETA 1734.0s\n",
      "done 41888/51968 | 6.0 chunks/s | ETA 1692.0s\n",
      "done 42144/51968 | 6.0 chunks/s | ETA 1650.1s\n",
      "done 42400/51968 | 6.0 chunks/s | ETA 1607.7s\n",
      "done 42656/51968 | 5.9 chunks/s | ETA 1565.1s\n",
      "done 42912/51968 | 5.9 chunks/s | ETA 1523.1s\n",
      "done 43168/51968 | 5.9 chunks/s | ETA 1480.5s\n",
      "done 43424/51968 | 5.9 chunks/s | ETA 1438.3s\n",
      "done 43680/51968 | 5.9 chunks/s | ETA 1395.8s\n",
      "done 43936/51968 | 5.9 chunks/s | ETA 1353.3s\n",
      "done 44192/51968 | 5.9 chunks/s | ETA 1310.1s\n",
      "done 44448/51968 | 5.9 chunks/s | ETA 1267.8s\n",
      "done 44704/51968 | 5.9 chunks/s | ETA 1225.0s\n",
      "done 44960/51968 | 5.9 chunks/s | ETA 1182.3s\n",
      "done 45216/51968 | 5.9 chunks/s | ETA 1139.5s\n",
      "done 45472/51968 | 5.9 chunks/s | ETA 1096.4s\n",
      "done 45728/51968 | 5.9 chunks/s | ETA 1053.3s\n",
      "done 45984/51968 | 5.9 chunks/s | ETA 1010.5s\n",
      "done 46240/51968 | 5.9 chunks/s | ETA 967.4s\n",
      "done 46496/51968 | 5.9 chunks/s | ETA 924.5s\n",
      "done 46752/51968 | 5.9 chunks/s | ETA 881.5s\n",
      "done 47008/51968 | 5.9 chunks/s | ETA 838.5s\n",
      "done 47264/51968 | 5.9 chunks/s | ETA 795.3s\n",
      "done 47520/51968 | 5.9 chunks/s | ETA 752.4s\n",
      "done 47776/51968 | 5.9 chunks/s | ETA 709.4s\n",
      "done 48032/51968 | 5.9 chunks/s | ETA 666.3s\n",
      "done 48288/51968 | 5.9 chunks/s | ETA 623.2s\n",
      "done 48544/51968 | 5.9 chunks/s | ETA 579.9s\n",
      "done 48800/51968 | 5.9 chunks/s | ETA 536.6s\n",
      "done 49056/51968 | 5.9 chunks/s | ETA 493.3s\n",
      "done 49312/51968 | 5.9 chunks/s | ETA 450.1s\n",
      "done 49568/51968 | 5.9 chunks/s | ETA 406.8s\n",
      "done 49824/51968 | 5.9 chunks/s | ETA 363.5s\n",
      "done 50080/51968 | 5.9 chunks/s | ETA 320.1s\n",
      "done 50336/51968 | 5.9 chunks/s | ETA 276.8s\n",
      "done 50592/51968 | 5.9 chunks/s | ETA 233.3s\n",
      "done 50848/51968 | 5.9 chunks/s | ETA 190.0s\n",
      "done 51104/51968 | 5.9 chunks/s | ETA 146.6s\n",
      "done 51360/51968 | 5.9 chunks/s | ETA 103.2s\n",
      "done 51616/51968 | 5.9 chunks/s | ETA 59.7s\n",
      "done 51872/51968 | 5.9 chunks/s | ETA 16.3s\n",
      "done 51968/51968 | 5.9 chunks/s | ETA 0.0s\n",
      "Saved: data/embeddings_fast/paraphrase-multilingual-MiniLM-L12-v2_384d_50k_float32.npy\n"
     ]
    }
   ],
   "source": [
    "# 1) Inputs & outputs\n",
    "texts = df_passages[\"chunk_text\"].astype(str).tolist()\n",
    "N = len(texts)\n",
    "\n",
    "# Choose batch size\n",
    "BATCH = 768 if DEVICE.type==\"cuda\" else 256   # tune: 512–1024 (GPU), 64–128 (CPU)\n",
    "OUT_DIR = \"data/embeddings_fast\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Determine embedding dimensionality once (dry run on 1 example)\n",
    "test_vec = embed_batch(add_passage_prefix([texts[0]]))\n",
    "DIM = test_vec.shape[1]\n",
    "\n",
    "# Use a memory-mapped array to write incrementally (resumable)\n",
    "mmap_path = os.path.join(OUT_DIR, f\"{MODEL_NAME.split('/')[-1]}_{DIM}d_50k_float32.mm\")\n",
    "embs = np.memmap(mmap_path, dtype=\"float32\", mode=\"w+\", shape=(N, DIM))\n",
    "\n",
    "# Optional: resume support — check how many rows already filled (NaNs if unwritten)\n",
    "# For a fresh run, start = 0. If resuming, detect start index from a sidecar file.\n",
    "start = 0\n",
    "sidecar = mmap_path + \".idx\"\n",
    "if os.path.exists(sidecar):\n",
    "    try:\n",
    "        start = int(open(sidecar).read().strip())\n",
    "    except Exception:\n",
    "        start = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(start, N, BATCH), desc=f\"Embedding on {DEVICE}\", unit=\"batch\"):\n",
    "    j = min(i + BATCH, N)\n",
    "    batch = add_passage_prefix(texts[i:j])\n",
    "    vecs = embed_batch(batch, max_len=TOKEN_BUDGET)\n",
    "    embs[i:j, :] = vecs\n",
    "    # Flush progress & write checkpoint index\n",
    "    embs.flush()\n",
    "    with open(sidecar, \"w\") as f:\n",
    "        f.write(str(j))\n",
    "    # Lightweight throughput display\n",
    "    done = j\n",
    "    dt = time.time() - t0\n",
    "    if dt > 0:\n",
    "        tqdm.write(f\"done {done}/{N} | {(done/dt):.1f} chunks/s | ETA {(N-done)/(done/dt+1e-9):.1f}s\")\n",
    "\n",
    "# Convert memmap to .npy cleanly\n",
    "final_npy = os.path.join(OUT_DIR, f\"{MODEL_NAME.split('/')[-1]}_{DIM}d_50k_float32.npy\")\n",
    "np.save(final_npy, np.asarray(embs))\n",
    "os.remove(sidecar)  # cleanup resume marker\n",
    "print(\"Saved:\", final_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5d609",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83289be",
   "metadata": {},
   "source": [
    "## <b>Indexing the embedded vectors</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4b57c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_passages must already exist with at least:\n",
    "# [\"doc_id\",\"chunk_id\",\"chunk_text\",\"preview\",\"lang\",\"_site\",\"title\",\"chunk_tokens\"]\n",
    "\n",
    "# 1) Stable global ID to align all stores (FAISS / Elasticsearch / SQLite)\n",
    "df_passages = df_passages.copy()\n",
    "df_passages[\"global_chunk_id\"] = (\n",
    "    df_passages[\"doc_id\"].astype(str) + \":\" + df_passages[\"chunk_id\"].astype(int).astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fcb685d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>_site</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>chunk_tokens</th>\n",
       "      <th>sent_start</th>\n",
       "      <th>sent_end</th>\n",
       "      <th>preview</th>\n",
       "      <th>global_chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>0:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>1:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>en</td>\n",
       "      <td>responsibility, this is her opportunity to als...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>responsibility, this is her opportunity to als...</td>\n",
       "      <td>1:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>hy</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "      <td>2:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>hy</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "      <td>3:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  chunk_id _site                                          title lang  \\\n",
       "0       0         0  None                           Երբեք չէի պատկերացնի   en   \n",
       "1       1         0  None                           Երբեք չէի պատկերացնի   en   \n",
       "2       1         1  None                           Երբեք չէի պատկերացնի   en   \n",
       "3       2         0  None  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   hy   \n",
       "4       3         0  None  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   hy   \n",
       "\n",
       "                                          chunk_text  chunk_tokens  \\\n",
       "0  \"I have never thought that I can do important ...           288   \n",
       "1  We spoke to Heghine for a long time and she of...           350   \n",
       "2  responsibility, this is her opportunity to als...            68   \n",
       "3  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...           174   \n",
       "4  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...           181   \n",
       "\n",
       "   sent_start  sent_end                                            preview  \\\n",
       "0           0         0  \"I have never thought that I can do important ...   \n",
       "1           0         0  We spoke to Heghine for a long time and she of...   \n",
       "2           0         0  responsibility, this is her opportunity to als...   \n",
       "3           0         0  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...   \n",
       "4           0         0  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...   \n",
       "\n",
       "  global_chunk_id  \n",
       "0             0:0  \n",
       "1             1:0  \n",
       "2             1:1  \n",
       "3             2:0  \n",
       "4             3:0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_passages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec7ca5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose the columns you’ll want at retrieval time (add more if you need)\n",
    "meta_cols = [\n",
    "    \"global_chunk_id\", \"doc_id\", \"chunk_id\", \"_site\", \"lang\",\n",
    "    \"title\", \"preview\", \"chunk_tokens\"\n",
    "]\n",
    "meta = df_passages[meta_cols].rename(columns={\"_site\":\"site\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64b16f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_chunk_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>site</th>\n",
       "      <th>lang</th>\n",
       "      <th>title</th>\n",
       "      <th>preview</th>\n",
       "      <th>chunk_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0:0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1:1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>Երբեք չէի պատկերացնի</td>\n",
       "      <td>responsibility, this is her opportunity to als...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2:0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>hy</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3:0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>hy</td>\n",
       "      <td>Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում</td>\n",
       "      <td>Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  global_chunk_id  doc_id  chunk_id  site lang  \\\n",
       "0             0:0       0         0  None   en   \n",
       "1             1:0       1         0  None   en   \n",
       "2             1:1       1         1  None   en   \n",
       "3             2:0       2         0  None   hy   \n",
       "4             3:0       3         0  None   hy   \n",
       "\n",
       "                                           title  \\\n",
       "0                           Երբեք չէի պատկերացնի   \n",
       "1                           Երբեք չէի պատկերացնի   \n",
       "2                           Երբեք չէի պատկերացնի   \n",
       "3  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   \n",
       "4  Աղետներին պատրաստ դպրոց` սահմանամերձ գյուղում   \n",
       "\n",
       "                                             preview  chunk_tokens  \n",
       "0  \"I have never thought that I can do important ...           288  \n",
       "1  We spoke to Heghine for a long time and she of...           350  \n",
       "2  responsibility, this is her opportunity to als...            68  \n",
       "3  Տավուշի մարզի Ներքին Ծաղկավան գյուղի դպրոցի VI...           174  \n",
       "4  Վերջին երեք տարիներին ՅՈՒՆԻՍԵՖ-ն այս ուղղությա...           181  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8af37a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meta: data/embeddings_fast/paraphrase-multilingual-MiniLM-L12-v2_passages_meta.parquet | rows: 51968\n"
     ]
    }
   ],
   "source": [
    "# 3) Save to Parquet (this is the file you’ll later load as meta)\n",
    "MODEL_TAG = \"paraphrase-multilingual-MiniLM-L12-v2\"   # or your chosen model name\n",
    "out_dir = \"data/embeddings_fast\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "meta_path = os.path.join(out_dir, f\"{MODEL_TAG}_passages_meta.parquet\")\n",
    "meta.to_parquet(meta_path, index=False)\n",
    "print(\"Saved meta:\", meta_path, \"| rows:\", len(meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d2d43",
   "metadata": {},
   "source": [
    "#### 1) Dense index (Semantic) with FAISS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909a398",
   "metadata": {},
   "source": [
    "FAISS is perfect for 50K - 500K vectors on one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c388498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file_name = f\"{MODEL_NAME.split('/')[-1]}_{DIM}d_50k_float32.npy\"\n",
    "metadata_file_name = f\"{MODEL_TAG}_passages_meta.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b64a62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, faiss\n",
    "\n",
    "# Load your embeddings and metadata (from earlier step)\n",
    "emb = np.load(f\"data/embeddings_fast/{embedding_file_name}\").astype(\"float32\")  # (N, d) L2-normalized\n",
    "meta = pd.read_parquet(f\"data/embeddings_fast/{metadata_file_name}\")  # includes global_chunk_id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5a020238",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)   # use IP since vectors are L2-normalized -> cosine\n",
    "index.add(emb)\n",
    "\n",
    "faiss.write_index(index, \"data/faiss_index/passages_flatip.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b08158",
   "metadata": {},
   "source": [
    "Query side:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2815fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_search(query_vecs: np.ndarray, k=10):\n",
    "    # query_vecs must be L2-normalized float32 (m x d)\n",
    "    D, I = index.search(query_vecs.astype(\"float32\"), k)\n",
    "    # Map to metadata rows\n",
    "    hits = []\n",
    "    for q, (scores, idxs) in enumerate(zip(D, I)):\n",
    "        rows = meta.iloc[idxs].copy()\n",
    "        rows[\"dense_score\"] = scores\n",
    "        rows[\"q\"] = q\n",
    "        hits.append(rows)\n",
    "    return pd.concat(hits, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dbe72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2876b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59aae74",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01d5c3",
   "metadata": {},
   "source": [
    "# <b>Keyword based search</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9414d6",
   "metadata": {},
   "source": [
    "### <b>2. Prepare data & bulk-ingest passages</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83e550d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure stable IDs\n",
    "df_passages = df_passages.copy()\n",
    "df_passages[\"global_chunk_id\"] = (\n",
    "    df_passages[\"doc_id\"].astype(str) + \":\" + df_passages[\"chunk_id\"].astype(int).astype(str)\n",
    ")\n",
    "\n",
    "# Minimal columns for ES\n",
    "to_index = df_passages.rename(columns={\"_site\": \"site\"})[\n",
    "    [\"global_chunk_id\",\"doc_id\",\"chunk_id\",\"site\",\"lang\",\"title\",\"chunk_text\",\"preview\",\"chunk_tokens\"]\n",
    "].fillna({\"title\":\"\", \"preview\":\"\", \"chunk_text\":\"\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "935efe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/x82plb356ylfzf1n3t44k5_40000gn/T/ipykernel_78742/2481316523.py:17: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  helpers.bulk(es, gen_actions(to_index), request_timeout=180)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"DfZP9TzO\")   # 👈 add this\n",
    ")\n",
    "\n",
    "def gen_actions(df):\n",
    "    for r in df.itertuples(index=False):\n",
    "        yield {\n",
    "            \"_index\": \"passages_bm25\",\n",
    "            \"_id\": r.global_chunk_id,\n",
    "            \"_source\": r._asdict()\n",
    "        }\n",
    "\n",
    "# Ingest\n",
    "helpers.bulk(es, gen_actions(to_index), request_timeout=180)\n",
    "es.indices.refresh(index=\"passages_bm25\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8863941",
   "metadata": {},
   "source": [
    "### <b>3. Keyword search functions (single & batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff501f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query_str: str, k: int = 50) -> pd.DataFrame:\n",
    "    body = {\n",
    "      \"size\": k,\n",
    "      \"query\": { \"multi_match\": {\n",
    "        \"query\": query_str,\n",
    "        \"fields\": [\"title^2\", \"chunk_text\"]\n",
    "      }}\n",
    "    }\n",
    "    res = es.search(index=\"passages_bm25\", body=body)\n",
    "    rows = []\n",
    "    for rank, hit in enumerate(res[\"hits\"][\"hits\"], start=1):\n",
    "        src = hit[\"_source\"]\n",
    "        rows.append({\n",
    "            \"query\": query_str,\n",
    "            \"rank\": rank,\n",
    "            \"sparse_score\": hit[\"_score\"],\n",
    "            \"global_chunk_id\": src[\"global_chunk_id\"],\n",
    "            \"doc_id\": src[\"doc_id\"],\n",
    "            \"chunk_id\": src[\"chunk_id\"],\n",
    "            \"title\": src.get(\"title\",\"\"),\n",
    "            \"site\": src.get(\"site\",\"\"),\n",
    "            \"lang\": src.get(\"lang\",\"\"),\n",
    "            \"preview\": src.get(\"preview\",\"\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def bm25_search_batch(queries: list[str], k: int = 50) -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    for qid, q in enumerate(queries):\n",
    "        df = bm25_search(q, k=k)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df.insert(0, \"query_id\", qid)\n",
    "        all_rows.append(df)\n",
    "    return pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cc210",
   "metadata": {},
   "source": [
    "#### <b>4. Convert chunk results → doc results (best chunk per doc) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00b53fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_doc_level(df_hits: pd.DataFrame, top_k_docs: int = 10) -> pd.DataFrame:\n",
    "    # keep best chunk per (query_id, doc_id)\n",
    "    sub = (df_hits\n",
    "           .sort_values([\"query_id\",\"doc_id\",\"sparse_score\"], ascending=[True, True, False])\n",
    "           .groupby([\"query_id\",\"doc_id\"], as_index=False)\n",
    "           .first())\n",
    "    # rerank docs per query\n",
    "    sub[\"rank\"] = sub.groupby(\"query_id\")[\"sparse_score\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    sub = sub.sort_values([\"query_id\",\"rank\"]).groupby(\"query_id\").head(top_k_docs).reset_index(drop=True)\n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be64a8e",
   "metadata": {},
   "source": [
    "### <b> 5. Run the baseline for a list of queries.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ebb5a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: directly pass a python list\n",
    "queries = [\n",
    "    \"child immunization reduces mortality\",\n",
    "    \"política de vacunación infantil\",\n",
    "    \"टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण\",\n",
    "    \"儿童免疫接种 对 死亡率 的 影响\"\n",
    "]\n",
    "\n",
    "# Option B: load from CSV with mapping (query_id, query_text)\n",
    "# queries_df = pd.read_csv(\"data/queries.csv\")\n",
    "# queries = queries_df[\"query_text\"].tolist()\n",
    "\n",
    "df_sparse_chunks = bm25_search_batch(queries, k=50)\n",
    "df_sparse_docs   = bm25_doc_level(df_sparse_chunks, top_k_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7544a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>sparse_score</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>1</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15900</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>चिकित्सा के क्षेत्र में टीकाकरण का अहम योगदान ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>2</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15901</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>टीकाकरण जिंदगियां बचाता है मॉर्डन टीकों और ओरल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>3</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15902</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>टीकाकरण अगली पीढ़ी की रक्षा करता है टीकाकरण ने...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>4</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15903</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>पैसे बचाने और आर्थिक स्थिति ठीक रखने में मददगा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>5</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15904</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>समय पर टीकाकरण करवा कर बच्चों में रोकी जा सकती...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>6</td>\n",
       "      <td>162.911960</td>\n",
       "      <td>15905</td>\n",
       "      <td>0</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>hi</td>\n",
       "      <td>आपके परिजनों और दोस्तों की सुरक्षा के लिए टीका...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>7</td>\n",
       "      <td>53.344685</td>\n",
       "      <td>15911</td>\n",
       "      <td>1</td>\n",
       "      <td>बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...</td>\n",
       "      <td>hi</td>\n",
       "      <td>) और मातृ संचारित हेपेटाइटिस बी (हेप बी टीका) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>8</td>\n",
       "      <td>43.010044</td>\n",
       "      <td>15912</td>\n",
       "      <td>0</td>\n",
       "      <td>बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...</td>\n",
       "      <td>hi</td>\n",
       "      <td>कुछ टीके उन बच्चों को क्यों नहीं लगाए जाते जो ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>9</td>\n",
       "      <td>42.758530</td>\n",
       "      <td>15925</td>\n",
       "      <td>0</td>\n",
       "      <td>बाधाओं से परे घर-घर टीकाकरण करने पहुंचते हैं स...</td>\n",
       "      <td>hi</td>\n",
       "      <td>राजस्थान के सिरोही जिले में गरासिया नामक आदिवा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2</td>\n",
       "      <td>टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण</td>\n",
       "      <td>10</td>\n",
       "      <td>40.111618</td>\n",
       "      <td>15910</td>\n",
       "      <td>0</td>\n",
       "      <td>बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...</td>\n",
       "      <td>hi</td>\n",
       "      <td>भारत में टीकाकरण ने न सिर्फ समुदायों को चिकित्...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id                                        query  rank  \\\n",
       "100         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     1   \n",
       "101         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     2   \n",
       "102         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     3   \n",
       "103         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     4   \n",
       "104         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     5   \n",
       "105         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     6   \n",
       "106         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     7   \n",
       "107         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     8   \n",
       "108         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण     9   \n",
       "109         2  टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण    10   \n",
       "\n",
       "     sparse_score  doc_id  chunk_id  \\\n",
       "100    162.911960   15900         0   \n",
       "101    162.911960   15901         0   \n",
       "102    162.911960   15902         0   \n",
       "103    162.911960   15903         0   \n",
       "104    162.911960   15904         0   \n",
       "105    162.911960   15905         0   \n",
       "106     53.344685   15911         1   \n",
       "107     43.010044   15912         0   \n",
       "108     42.758530   15925         0   \n",
       "109     40.111618   15910         0   \n",
       "\n",
       "                                                 title lang  \\\n",
       "100        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "101        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "102        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "103        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "104        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "105        टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण   hi   \n",
       "106  बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...   hi   \n",
       "107  बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...   hi   \n",
       "108  बाधाओं से परे घर-घर टीकाकरण करने पहुंचते हैं स...   hi   \n",
       "109  बच्चों के टीकाकरण से संबंधित अक्सर पूछे जाने व...   hi   \n",
       "\n",
       "                                               preview  \n",
       "100  चिकित्सा के क्षेत्र में टीकाकरण का अहम योगदान ...  \n",
       "101  टीकाकरण जिंदगियां बचाता है मॉर्डन टीकों और ओरल...  \n",
       "102  टीकाकरण अगली पीढ़ी की रक्षा करता है टीकाकरण ने...  \n",
       "103  पैसे बचाने और आर्थिक स्थिति ठीक रखने में मददगा...  \n",
       "104  समय पर टीकाकरण करवा कर बच्चों में रोकी जा सकती...  \n",
       "105  आपके परिजनों और दोस्तों की सुरक्षा के लिए टीका...  \n",
       "106  ) और मातृ संचारित हेपेटाइटिस बी (हेप बी टीका) ...  \n",
       "107  कुछ टीके उन बच्चों को क्यों नहीं लगाए जाते जो ...  \n",
       "108  राजस्थान के सिरोही जिले में गरासिया नामक आदिवा...  \n",
       "109  भारत में टीकाकरण ने न सिर्फ समुदायों को चिकित्...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "df_sparse_chunks[df_sparse_chunks['query'] == 'टीकाकरण क्यों जरूरी है इसके पांच मुख्य कारण'].head(10)[[\"query_id\",\"query\",\"rank\",\"sparse_score\",\"doc_id\",\"chunk_id\",\"title\",\"lang\",\"preview\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4a133",
   "metadata": {},
   "source": [
    "### 6) Evaluate (re-using your earlier evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ae4d8",
   "metadata": {},
   "source": [
    "If you’ve plugged in the evaluation helpers I gave you, this is straightforward. We’ll evaluate at doc-level (recommended for document search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ab3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth CSV with columns: query_id, doc_id, relevant\n",
    "GT_PATH = \"data/ground/ground_truth_sample.csv\"\n",
    "\n",
    "# Reuse the evaluate_retrieval() function you already have\n",
    "per_q, summary = evaluate_retrieval(\n",
    "    df_hits=df_sparse_docs.rename(columns={\"sparse_score\":\"score\"}),  # evaluator expects 'score' or 'dense_score'\n",
    "    gt_path_or_df=GT_PATH,\n",
    "    id_level=\"doc\",       # evaluate documents\n",
    "    k_list=(1,3,5,10),\n",
    "    gain_scheme=\"exp2\"\n",
    ")\n",
    "\n",
    "display(per_q.head())\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse_chunks.to_parquet(\"eval/sparse_chunks.parquet\", index=False)\n",
    "df_sparse_docs.to_parquet(\"eval/sparse_docs.parquet\", index=False)\n",
    "per_q.to_parquet(\"eval/metrics_sparse_per_query.parquet\", index=False)\n",
    "summary.to_csv(\"eval/metrics_sparse_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae5464",
   "metadata": {},
   "source": [
    "## <b>Project preparation steps</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d42a7e",
   "metadata": {},
   "source": [
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f18b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import emoji\n",
    "import math\n",
    "\n",
    "# Nltk libraries for text cleaning and processing.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import stopwordsiso as stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Import libraries for text cleaning.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Thread pooling.\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "# Import system specific libraries.\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import fast text library for language detection.\n",
    "import fasttext\n",
    "\n",
    "# Import libraries for performance evaluation and measurements.\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Import FAISS library for indexing embedded vectors.\n",
    "import faiss\n",
    "\n",
    "# Sentence transformer based models.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import pickle for saving and loading objects.\n",
    "import pickle\n",
    "\n",
    "# Import sqlite3 library for storing metadata.\n",
    "import sqlite3\n",
    "\n",
    "# Ignore future and deprecated warnings to get cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Logs.\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required nltk packages.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4508e15",
   "metadata": {},
   "source": [
    "### Configuration logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"pipeline.log\",         # log file name\n",
    "    level=logging.INFO,              # logging level (INFO, DEBUG, ERROR)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # log format\n",
    "    filemode=\"w\"                     # overwrite log file each run (\"a\" to append)\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8e426",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIRECTORY_PATH = \"config\"\n",
    "DATASET_DIRECTORY_PATH = \"datasets\"\n",
    "DATA_DIRECTORY_PATH = \"data\"\n",
    "MULTILINGUAL_DOCUMENTS_DIRECTORY_PATH = \"datasets/multilingual_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e023c",
   "metadata": {},
   "source": [
    "### Helping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d1fd7",
   "metadata": {},
   "source": [
    "#### 1. Load configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project specific configuration file.\n",
    "def load_config(filename):\n",
    "    config_file_path = f\"{CONFIG_DIRECTORY_PATH}/{filename}.yml\"\n",
    "    with open(config_file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Return config file.\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e16d0",
   "metadata": {},
   "source": [
    "#### 2. Get Language detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4680f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained language detection model.\n",
    "def get_langauge_detection_model(language_detection_config):\n",
    "    model = language_detection_config['model']\n",
    "    pre_trained_model_filepath = f\"{DATA_DIRECTORY_PATH}/{model}\"\n",
    "    if not os.path.exists(pre_trained_model_filepath):\n",
    "        raise FileNotFoundError(f\"{pre_trained_model_filepath} not found. Download it from model's website.\")\n",
    "    else:\n",
    "        return fasttext.load_model(pre_trained_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fba2c3",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b86b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configured data.\n",
    "site_metadata_config = load_config('sites-metadata');\n",
    "project_config = load_config('project');\n",
    "\n",
    "# Load fast track model for language detection.\n",
    "fast_track_language_detection_model = get_langauge_detection_model(project_config['language_detection']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6cfc4",
   "metadata": {},
   "source": [
    "### 3. Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    return project_config['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565633f2",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757eb3f",
   "metadata": {},
   "source": [
    "# <b>Text preprocessing pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9b674",
   "metadata": {},
   "source": [
    "### Helper functions to preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f4272",
   "metadata": {},
   "source": [
    "#### 1. Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text to remove html formattings, emojis, puntuations and normalize spaces.\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove html formats.\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove emojis.\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Remove puntuations.\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    \n",
    "    # Remove normalize spaces.\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Remove url links.\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082fd96",
   "metadata": {},
   "source": [
    "#### 2. Text normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text.\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0fedc",
   "metadata": {},
   "source": [
    "#### 3. Tokenization and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d44205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts in sentences and words, and remove stopwords.\n",
    "def tokenize_and_filter(row, axis = 1):\n",
    "    text = row['cleaned_text']\n",
    "    lang = row['language']\n",
    "\n",
    "    # Sentence tokenize.\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Word tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords if available for that language.\n",
    "    if stopwords.has_lang(lang):\n",
    "        sw = stopwords.stopwords(lang)\n",
    "        words = [w for w in words if w not in sw]\n",
    "\n",
    "    return pd.Series({\"sentences\": sentences, \"tokens\": words})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e79537",
   "metadata": {},
   "source": [
    "#### 4. Comibined all processeing steps in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f29f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Text Cleaning ----------------\n",
    "def process_text(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2acfad",
   "metadata": {},
   "source": [
    "#### 4. Language detection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Single-Batch Language Detection ----------------\n",
    "def detect_language_batch(text_batch):\n",
    "    labels, _ = fast_track_language_detection_model.predict(text_batch, k=1)\n",
    "    return [lbl[0].replace(\"__label__\", \"\") if lbl else \"unknown\" for lbl in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503583d",
   "metadata": {},
   "source": [
    "4. Thread based batch language detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Threaded Batch Language Detection ----------------\n",
    "def batch_detect_language_parallel(texts, batch_size=1000):\n",
    "    chunks = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    languages = []\n",
    "\n",
    "    with Pool() as pool:  # ThreadPool\n",
    "        for batch_result in tqdm(pool.imap(detect_language_batch, chunks), total=len(chunks), desc=\"Language Detection\"):\n",
    "            languages.extend(batch_result)\n",
    "    \n",
    "    return languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77e867",
   "metadata": {},
   "source": [
    "#### 5. Preprocessing single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae60313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Preprocess Single DataFrame ----------------\n",
    "def preprocess_dataframe_parallel(df, metadata):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a text column, returns new Dataframe with \n",
    "    cleaned, normalized, language, tokens.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    text_column = metadata['text_column']\n",
    "    batch_size = metadata['batch_size']\n",
    "    \n",
    "    # Remove duplicates\n",
    "    tqdm.pandas(desc=\"Removing duplicates....\")\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    duplicates_removed = initial_count - len(df)\n",
    "    \n",
    "    # Clean text with progress bar\n",
    "    tqdm.pandas(desc=\"Cleaning text....\")\n",
    "    df['cleaned_text'] = df[text_column].progress_apply(process_text)\n",
    "\n",
    "    \n",
    "    # Remove empty cleaned text\n",
    "    df = df[df['cleaned_text'].notna()].reset_index(drop=True)\n",
    "    \n",
    "    # Detect languages in parallel\n",
    "    texts = df['cleaned_text'].tolist()\n",
    "    df['language'] = batch_detect_language_parallel(texts, batch_size=batch_size)\n",
    "\n",
    "    # Remove unknown languages\n",
    "    df = df[df['language'] != \"unknown\"].reset_index(drop=True)\n",
    "\n",
    "    # tokenization, processin.\n",
    "    tqdm.pandas(desc=\"Tokenizing text....\")\n",
    "    df_tokens = df.progress_apply(tokenize_and_filter, axis = 1)\n",
    "    df = df.join(df_tokens)\n",
    "    \n",
    "    # Capture site stats\n",
    "    site_stats = {\n",
    "        \"total_rows\": initial_count,\n",
    "        \"duplicates_removed\": duplicates_removed,\n",
    "        \"rows_kept\": len(df),\n",
    "        \"languages_detected\": df['language'].unique().tolist()\n",
    "    }\n",
    "    \n",
    "    return df, site_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf16c1",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21613c",
   "metadata": {},
   "source": [
    "# <b> Data preparation pipeline.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19652182",
   "metadata": {},
   "source": [
    "### Multiple csv files reading and processing with summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Multi-Site CSV Pipeline with Summary ----------------\n",
    "def process_all_sites_with_summary(metadata):\n",
    "\n",
    "    # Get list of sites.\n",
    "    sites = site_metadata_config.get('sites', [])\n",
    "\n",
    "    # Prepare return variables.\n",
    "    all_dfs = []\n",
    "    summary_list = []\n",
    "\n",
    "    for site_csv in sites:\n",
    "        csv_directory = f\"{MULTILINGUAL_DOCUMENTS_DIRECTORY_PATH}/{site_csv}\"\n",
    "        print(csv_directory)\n",
    "\n",
    "        # ---------------- Get list of CSV files ----------------\n",
    "        csv_files = glob.glob(os.path.join(csv_directory, \"*.csv\"))\n",
    "        logger.info(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "        # ---------------- Read all CSVs and combine ----------------\n",
    "        for file in csv_files:\n",
    "            if os.path.exists(file):\n",
    "                input_df = pd.read_csv(file)\n",
    "                df = input_df.copy()\n",
    "                logger.info(f\"[INFO] Processing site: {site_csv} ({len(df)} rows)\")\n",
    "\n",
    "                # Trigger cleaning of dataframes.\n",
    "                df_cleaned, site_stats = preprocess_dataframe_parallel(df, metadata)\n",
    "                logger.info(f\"[INFO] Done {site_csv}: {site_stats['duplicates_removed']} duplicates removed, {site_stats['rows_kept']} rows kept\")\n",
    "                site_stats[\"site\"] = site_csv\n",
    "                summary_list.append(site_stats)\n",
    "                all_dfs.append(df_cleaned)\n",
    "            else:\n",
    "                \n",
    "                logger.info(f\"[WARNING] File not found: {file}\")\n",
    "    \n",
    "    # Merge all cleaned DataFrames\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged_df = merged_df.drop_duplicates(subset=[metadata['text_column'], 'cleaned_text']).reset_index(drop=True)\n",
    "        logger.info(f\"[INFO] Merged DataFrame contains {len(merged_df)} unique rows after deduplication\")\n",
    "        \n",
    "        # Save if requested\n",
    "        save_path = f\"{DATA_DIRECTORY_PATH}/{metadata['processed_file_name']}.csv\"\n",
    "        save_format = metadata['data_file_format']\n",
    "        if save_path:\n",
    "            if save_format.lower() == \"csv\":\n",
    "                merged_df.to_csv(save_path, index=False)\n",
    "            elif save_format.lower() == \"parquet\":\n",
    "                merged_df.to_parquet(save_path, index=False)\n",
    "            else:\n",
    "                print(f\"[WARNING] Unknown save_format '{save_format}'. Skipping save.\")\n",
    "            logger.info(f\"[INFO] Saved merged DataFrame to {save_path}\")\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_list)\n",
    "        print(\"\\n[INFO] Site Summary Table:\")\n",
    "        print(summary_df)\n",
    "        \n",
    "        return merged_df, summary_df\n",
    "    else:\n",
    "        print(\"[INFO] No valid data found in any site CSVs.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62411833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of sites and candidate languages configurations.\n",
    "metadata = get_metadata()\n",
    "\n",
    "# Trigger pipeline.\n",
    "processed_df, summary_df = process_all_sites_with_summary(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209cf82",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa6443",
   "metadata": {},
   "source": [
    "#### Sites\n",
    "1. Global (https://www.unicef.org/)\n",
    "2. Armenia (https://www.unicef.org/armenia/)\n",
    "3. Bangladesh (https://www.unicef.org/bangladesh/)\n",
    "4. Cambodia (https://www.unicef.org/cambodia/)\n",
    "5. China (https://www.unicef.org/china/)\n",
    "6. ECA (https://www.unicef.org/eca/)\n",
    "7. India (https://www.unicef.org/india/)\n",
    "8. Myanmar (https://www.unicef.org/myanmar)\n",
    "9. Peru (https://www.unicef.org/peru/)\n",
    "10. Vietnam (https://www.unicef.org/vietnam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb51f8",
   "metadata": {},
   "source": [
    "#### Low level Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6e2fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbef921",
   "metadata": {},
   "source": [
    "#### Collect text data of press releases and articles from all candidate sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a460b51",
   "metadata": {},
   "source": [
    "#### Get CSV data into dataframe and execute pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f869c1",
   "metadata": {},
   "source": [
    "1. Read data from csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe58d53",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964930b",
   "metadata": {},
   "source": [
    "## Load processed and cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{DATA_DIRECTORY_PATH}/{project_config['metadata']['processed_file_name']}.csv\"\n",
    "df = pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc671cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import ftfy\n",
    "import langid\n",
    "from tqdm.auto import tqdm\n",
    "from blingfire import text_to_sentences, text_to_words\n",
    "\n",
    "# ---- Optional tokenizers (if available) for better segmentation in some languages\n",
    "try:\n",
    "    import jieba             # zh\n",
    "except Exception:\n",
    "    jieba = None\n",
    "\n",
    "try:\n",
    "    from fugashi import Tagger  # ja\n",
    "    _ja_tagger = Tagger() if 'Tagger' in globals() else None\n",
    "except Exception:\n",
    "    _ja_tagger = None\n",
    "\n",
    "try:\n",
    "    import pythainlp.tokenize as thai_tok  # th\n",
    "except Exception:\n",
    "    thai_tok = None\n",
    "\n",
    "tqdm.pandas()  # enable progress bars on DataFrame.apply\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "CTRL_RE = re.compile(r'[\\u0000-\\u001f\\u007f\\u200b\\u200c\\u200d]')  # control + ZW* chars\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Unicode-safe normalization & light cleaning that respects multilingual scripts.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Fix mojibake & odd encodings, normalize Unicode\n",
    "    s = ftfy.fix_text(text)\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # Strip HTML if any\n",
    "    s = BeautifulSoup(s, \"html.parser\").get_text(\" \")\n",
    "\n",
    "    # Remove URLs (optional): replace with a token so sentence boundaries remain sane\n",
    "    s = URL_RE.sub(\" <URL> \", s)\n",
    "\n",
    "    # Remove control chars (keep emojis & CJK intact)\n",
    "    s = CTRL_RE.sub(\" \", s)\n",
    "\n",
    "    # Collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # DO NOT lowercase by default (can harm German ß, Turkish I/ı, proper nouns, etc.)\n",
    "    # If you need case-insensitive search later, use .casefold() at query time.\n",
    "    return s\n",
    "\n",
    "def detect_lang(text: str) -> str:\n",
    "    \"\"\"Fast, offline language detection.\"\"\"\n",
    "    if not text:\n",
    "        return \"unk\"\n",
    "    code, _ = langid.classify(text)\n",
    "    return code\n",
    "\n",
    "def sent_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Multilingual sentence segmentation via blingfire (robust & very fast).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    # blingfire returns '\\n' separated sentences\n",
    "    sents = text_to_sentences(text).split(\"\\n\")\n",
    "    # Clean up any blanks\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def word_tokenize(text: str, lang: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Word tokenization with multilingual fallbacks:\n",
    "    - zh: jieba if available, else blingfire\n",
    "    - ja: fugashi if available, else blingfire\n",
    "    - th: PyThaiNLP if available, else blingfire\n",
    "    - others: blingfire\n",
    "    (blingfire respects CJK/emoji and is a sensible default.)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    lang = (lang or \"\").split(\"_\")[0]  # normalize like 'zh', 'ja', 'th', 'en', etc.\n",
    "\n",
    "    if lang == \"zh\" and jieba is not None:\n",
    "        return [t.strip() for t in jieba.cut(text) if t.strip()]\n",
    "    if lang == \"ja\" and _ja_tagger is not None:\n",
    "        return [w.surface for w in _ja_tagger(text) if w.surface.strip()]\n",
    "    if lang == \"th\" and thai_tok is not None:\n",
    "        return [t.strip() for t in thai_tok.word_tokenize(text) if t.strip()]\n",
    "\n",
    "    # Default: blingfire word breaker -> returns a space-separated string\n",
    "    return [w for w in text_to_words(text).split() if w]\n",
    "\n",
    "def preprocess_row(row):\n",
    "    \"\"\"\n",
    "    Apply the full pipeline to a single row with columns:\n",
    "    - 'title' (kept as-is)\n",
    "    - 'text'  (processed)\n",
    "    Returns a dict to be expanded into new columns.\n",
    "    \"\"\"\n",
    "    raw = row.get(\"text\", \"\")\n",
    "    clean = normalize_text(raw)\n",
    "    lang = detect_lang(clean) if clean else \"unk\"\n",
    "    sents = sent_tokenize(clean)\n",
    "    tokens = word_tokenize(clean, lang)\n",
    "\n",
    "    return {\n",
    "        \"ai_clean_text\": clean,\n",
    "        \"ai_lang\": lang,\n",
    "        \"ai_sentences\": sents,\n",
    "        \"ai_tokens\": tokens,\n",
    "        \"ai_n_sentences\": len(sents),\n",
    "        \"ai_n_tokens\": len(tokens),\n",
    "    }\n",
    "\n",
    "# ------------- Example usage -------------\n",
    "# Suppose you already have a DataFrame df with columns: 'title', 'text'\n",
    "# df = pd.read_csv(\"your_multilingual.csv\")  # for example\n",
    "\n",
    "# Process with a progress bar:\n",
    "# (If you see tokenizer parallelism warnings anywhere else in your stack, you can:\n",
    "#   import os; os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# They typically come from Hugging Face tokenizers, not used here.)\n",
    "results = df.progress_apply(preprocess_row, axis=1, result_type=\"expand\")\n",
    "\n",
    "# Merge back with original columns (keeping title/text)\n",
    "df_processed = pd.concat([df[[\"title\", \"text\"]], results], axis=1)\n",
    "\n",
    "# Optionally, persist efficiently:\n",
    "# df_processed.to_parquet(\"multilingual_processed.parquet\", index=False)\n",
    "# Or CSV (loses Python list types unless you json-encode them):\n",
    "# df_processed.to_json(\"multilingual_processed.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34695b",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5ef5c",
   "metadata": {},
   "source": [
    "## <b>Model Embedding Pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoids fork warning\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class EmbeddingPipeline:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", index_dir=\"./index_store\", batch_size=512, n_workers=4):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index_dir = index_dir\n",
    "        os.makedirs(index_dir, exist_ok=True)\n",
    "        self.index = None\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = None\n",
    "        self.texts = []\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def generate_embeddings_batch(self, texts, save_path=None):\n",
    "        \"\"\"Generate embeddings in batches, normalize for cosine similarity, optionally save to disk.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for start in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[start:start+self.batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "            faiss.normalize_L2(batch_embeddings)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "            print(f\"Processed batch {start}-{start+len(batch_texts)}\")\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        print(f\"Total embeddings shape: {all_embeddings.shape}\")\n",
    "\n",
    "        if save_path:\n",
    "            np.save(save_path, all_embeddings)\n",
    "            print(f\"Saved embeddings to {save_path}\")\n",
    "\n",
    "        self.dim = all_embeddings.shape[1]\n",
    "        self.texts = texts\n",
    "        return all_embeddings\n",
    "\n",
    "    def build_index(self, embeddings, index_type=\"flat\", **kwargs):\n",
    "        \"\"\"Build FAISS index (flat, hnsw, or ivf) with cosine similarity.\"\"\"\n",
    "        if self.dim is None:\n",
    "            self.dim = embeddings.shape[1]\n",
    "\n",
    "        if index_type == \"flat\":\n",
    "            self.index = faiss.IndexFlatIP(self.dim)\n",
    "\n",
    "        elif index_type == \"hnsw\":\n",
    "            M = kwargs.get(\"M\", 16)\n",
    "            efConstruction = kwargs.get(\"efConstruction\", 100)\n",
    "            self.index = faiss.IndexHNSWFlat(self.dim, M, faiss.METRIC_INNER_PRODUCT)\n",
    "            self.index.hnsw.efConstruction = efConstruction\n",
    "\n",
    "        elif index_type == \"ivf\":\n",
    "            nlist = kwargs.get(\"nlist\", 100)\n",
    "            quantizer = faiss.IndexFlatIP(self.dim)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, self.dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "            print(\"Training IVF index...\")\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported index type. Use 'flat', 'hnsw', or 'ivf'.\")\n",
    "\n",
    "        # Add in batches\n",
    "        for i in range(0, embeddings.shape[0], self.batch_size):\n",
    "            self.index.add(embeddings[i:i+self.batch_size])\n",
    "\n",
    "    def save_index(self, name=\"index.faiss\"):\n",
    "        path = os.path.join(self.index_dir, name)\n",
    "        faiss.write_index(self.index, path)\n",
    "        print(f\"Index saved at {path}\")\n",
    "\n",
    "    def load_index(self, name=\"index.faiss\"):\n",
    "        path = os.path.join(self.index_dir, name)\n",
    "        self.index = faiss.read_index(path)\n",
    "        print(f\"Index loaded from {path}\")\n",
    "\n",
    "    def search(self, query, top_k=5, as_df=True):\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        D, I = self.index.search(query_embedding, top_k)\n",
    "\n",
    "        if as_df:\n",
    "            results = []\n",
    "            for rank, (score, idx) in enumerate(zip(D[0], I[0]), start=1):\n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"rank\": rank,\n",
    "                    \"index\": int(idx),\n",
    "                    \"cosine_similarity\": float(score),\n",
    "                    \"text\": self.texts[idx] if self.texts else None\n",
    "                })\n",
    "            return pd.DataFrame(results)\n",
    "        return D, I\n",
    "\n",
    "    def search_batch(self, queries, top_k=5):\n",
    "        query_embeddings = self.model.encode(queries, convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embeddings)\n",
    "        D, I = self.index.search(query_embeddings, top_k)\n",
    "\n",
    "        all_results = []\n",
    "        for q_idx, query in enumerate(queries):\n",
    "            for rank, (score, idx) in enumerate(zip(D[q_idx], I[q_idx]), start=1):\n",
    "                all_results.append({\n",
    "                    \"query\": query,\n",
    "                    \"rank\": rank,\n",
    "                    \"index\": int(idx),\n",
    "                    \"cosine_similarity\": float(score),\n",
    "                    \"text\": self.texts[idx] if self.texts else None\n",
    "                })\n",
    "        return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EmbeddingPipeline(batch_size=1024, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98241080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate embeddings in batches\n",
    "embeddings = pipeline.generate_embeddings_batch(sentences.to_list(), save_path=\"embeddings_100k.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HNSW index using threads\n",
    "pipeline.build_index(embeddings, index_type=\"flat\", M=16, efConstruction=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Save index\n",
    "pipeline.save_index(\"faiss_hnsw_parallel.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702a7d5",
   "metadata": {},
   "source": [
    "### Load saved vector indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d463adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EmbeddingPipeline()\n",
    "pipeline.load(\"large_index.faiss\", \"metadata_large.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a628cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Search batch queries\n",
    "queries = [\"Unicef kids\", \"उम्मीदों की नई सुबह\", \"mundo\"]\n",
    "df_results = pipeline.search_batch(queries, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b531c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf137c",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b0f26",
   "metadata": {},
   "source": [
    "# Query Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f88bd",
   "metadata": {},
   "source": [
    "### Load index for query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later reload:\n",
    "index = faiss.read_index(\"paraphrase-multilingual-MiniLM-L12-v2__indexes.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40208d8",
   "metadata": {},
   "source": [
    "#### Query to indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae76c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query FAISS\n",
    "query = \"situation reports in hindi\"\n",
    "query_vec = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "D, I = index.search(query_vec, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f5260",
   "metadata": {},
   "source": [
    "## Print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200088b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch metadata from SQLite\n",
    "print(\"\\nSearch results:\")\n",
    "I[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "879c72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements_new.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c35fedbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env export --no-builds > environment.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645dbf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
