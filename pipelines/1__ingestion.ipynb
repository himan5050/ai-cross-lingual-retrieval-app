{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7ee7ca",
   "metadata": {},
   "source": [
    "# <b>Content ingestion pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8962f",
   "metadata": {},
   "source": [
    "### Document sources:\n",
    "- CSV files of public articles and press releases on multilingual humantarian sites.\n",
    "- These data sources often contain multilingual content with mixed formats (HTML, PDF, Plain text, structured data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0ffe8",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2ab46",
   "metadata": {},
   "source": [
    "### <b>Data preparation and text-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43ed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "nb_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0dd0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see \"tokenizers parallelism\" warnings from other libs, silence them:\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ac7cb",
   "metadata": {},
   "source": [
    "#### 1. Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f9adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import ftfy\n",
    "import langid\n",
    "from blingfire import text_to_sentences, text_to_words\n",
    "\n",
    "# Optional tokenizers (used if available)\n",
    "try:\n",
    "    import jieba  # zh\n",
    "except Exception:\n",
    "    jieba = None\n",
    "\n",
    "try:\n",
    "    from fugashi import Tagger  # ja\n",
    "    _ja_tagger = Tagger()\n",
    "except Exception:\n",
    "    _ja_tagger = None\n",
    "\n",
    "try:\n",
    "    import pythainlp.tokenize as thai_tok  # th\n",
    "except Exception:\n",
    "    thai_tok = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b165f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configure your paths here ----\n",
    "\n",
    "# folder containing many *.csv like site_a.csv, site_b.csv\n",
    "RAW_DOCS    = Path(\"../dataset/raw_docs\")                 \n",
    "\n",
    "# processed file\n",
    "OUT_PARQ  = Path(\"../shared-data-library/out/df_processed.parquet\")\n",
    "\n",
    "# e.g., Path(\"../shared-data-library/df_processed.parquet\") if you also want JSONL\n",
    "OUT_JSONL = None\n",
    "\n",
    "# 2k‚Äì10k works well; adjust to memory/CPU\n",
    "BATCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f381e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compiled regexes\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "CTRL_RE = re.compile(r'[\\u0000-\\u001f\\u007f\\u200b\\u200c\\u200d]')  # control + zero-width chars\n",
    "MULTISPACE_RE = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a22fe",
   "metadata": {},
   "source": [
    "#### Text preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e3c44",
   "metadata": {},
   "source": [
    "Text normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae07ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Unicode-safe normalization & light cleaning that respects multilingual scripts.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    s = ftfy.fix_text(text)                           # fix mojibake\n",
    "    s = unicodedata.normalize(\"NFC\", s)               # compose accents consistently\n",
    "    s = BeautifulSoup(s, \"html.parser\").get_text(\" \") # strip HTML safely\n",
    "    s = URL_RE.sub(\" <URL> \", s)                      # keep a URL placeholder (useful for boundaries)\n",
    "    s = CTRL_RE.sub(\" \", s)                           # drop control / zero-width chars\n",
    "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
    "    # Intentionally NOT lowercasing (Turkish I/ƒ±, German √ü, proper nouns)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61092f1",
   "metadata": {},
   "source": [
    "Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcffb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"unk\"\n",
    "    code, _ = langid.classify(text)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7271640",
   "metadata": {},
   "source": [
    "Sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1499765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    sents = text_to_sentences(text).split(\"\\n\")\n",
    "    return [s.strip() for s in sents if s.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d7e20",
   "metadata": {},
   "source": [
    "Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3206b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text: str, lang: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    lang = (lang or \"\").split(\"_\")[0]\n",
    "    if lang == \"zh\" and jieba is not None:\n",
    "        return [t.strip() for t in jieba.cut(text) if t.strip()]\n",
    "    if lang == \"ja\" and _ja_tagger is not None:\n",
    "        return [w.surface for w in _ja_tagger(text) if w.surface.strip()]\n",
    "    if lang == \"th\" and thai_tok is not None:\n",
    "        return [t.strip() for t in thai_tok.word_tokenize(text) if t.strip()]\n",
    "    # Default fast multilingual fallback\n",
    "    return [w for w in text_to_words(text).split() if w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06328bb",
   "metadata": {},
   "source": [
    "Preprocess for single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd53eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_row(row: Dict) -> Dict:\n",
    "    raw = row.get(\"text\", \"\")\n",
    "    clean = normalize_text(raw)\n",
    "    lang = detect_lang(clean) if clean else \"unk\"\n",
    "    sents = sent_tokenize(clean)\n",
    "    tokens = word_tokenize(clean, lang)\n",
    "    return {\n",
    "        \"site\": row.get(\"_site\", \"\"),\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"text\": raw,\n",
    "        \"clean_text\": clean,\n",
    "        \"lang\": lang,\n",
    "        \"sentences\": sents,\n",
    "        \"tokens\": tokens,\n",
    "        \"n_sentences\": len(sents),\n",
    "        \"n_tokens\": len(tokens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19800035",
   "metadata": {},
   "source": [
    "#### Load CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a7f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(in_dir: Path) -> pd.DataFrame:\n",
    "    paths = sorted(in_dir.glob(\"*.csv\"))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {in_dir.resolve()}\")\n",
    "\n",
    "    frames = []\n",
    "    for p in tqdm(paths, desc=\"Loading CSVs\"):\n",
    "        site = p.stem  # from \"site_name.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                p,\n",
    "                encoding=\"utf-8-sig\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                dtype={\"title\": \"string\", \"text\": \"string\"},\n",
    "                usecols=lambda c: c in (\"title\", \"text\"),\n",
    "            )\n",
    "        except Exception:\n",
    "            # Fallback if columns/encodings are messy\n",
    "            df = pd.read_csv(p, encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "            for col in (\"title\", \"text\"):\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            df = df[[\"title\", \"text\"]]\n",
    "\n",
    "        df[\"title\"] = df[\"title\"].astype(\"string\")\n",
    "        df[\"text\"]  = df[\"text\"].astype(\"string\")\n",
    "        df[\"_site\"] = site\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True)\n",
    "    # Drop rows where both title and text are empty\n",
    "    mask_empty = all_df[\"title\"].fillna(\"\").str.strip().eq(\"\") & all_df[\"text\"].fillna(\"\").str.strip().eq(\"\")\n",
    "    all_df = all_df[~mask_empty].reset_index(drop=True)\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f3d82",
   "metadata": {},
   "source": [
    "Remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44614602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dedupe_key(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a canonical key for deduplication:\n",
    "    - normalize with normalize_text (fix mojibake, NFC, strip HTML, keep <URL>, remove control chars)\n",
    "    - collapse whitespace\n",
    "    - casefold for robust, language-aware case-insensitivity\n",
    "    \"\"\"\n",
    "    s = normalize_text(s if isinstance(s, str) else \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b5b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_by_text(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    # Build key on the raw text (so we can drop before heavy processing)\n",
    "    df[\"__dedupe_key\"] = df[\"text\"].fillna(\"\").map(make_dedupe_key)\n",
    "\n",
    "    # Mark dupes; keep the first occurrence\n",
    "    dup_mask = df[\"__dedupe_key\"].duplicated(keep=\"first\")\n",
    "    n_dups = int(dup_mask.sum())\n",
    "\n",
    "    df_dups   = df.loc[dup_mask, [\"_site\", \"title\", \"text\", \"__dedupe_key\"]].reset_index(drop=True)\n",
    "    df_nodup  = df.loc[~dup_mask].drop(columns=[\"__dedupe_key\"]).reset_index(drop=True)\n",
    "\n",
    "    return df_nodup, df_dups, n_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a75f72",
   "metadata": {},
   "source": [
    "#### Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7cb8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_batched(df: pd.DataFrame, batch_size: int = 5000) -> pd.DataFrame:\n",
    "    df = df[[\"_site\", \"title\", \"text\"]].copy()\n",
    "    n = len(df)\n",
    "    results = []\n",
    "\n",
    "    for start in tqdm(range(0, n, batch_size), desc=\"Processing batches\"):\n",
    "        end = min(n, start + batch_size)\n",
    "        records = df.iloc[start:end].to_dict(orient=\"records\")\n",
    "        out = [preprocess_row(r) for r in records]\n",
    "        part = pd.DataFrame(out)\n",
    "        part[\"lang\"] = part[\"lang\"].astype(\"category\")\n",
    "        results.append(part)\n",
    "\n",
    "    processed = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # If memory becomes an issue, consider these toggles:\n",
    "    # processed[\"tokens\"] = processed[\"tokens\"].apply(lambda t: \" \".join(t))  # store tokens as a single string\n",
    "    # processed = processed.drop(columns=[\"sentences\"])                        # drop sentence list\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c6c080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcad8697b4d943e29e2ffb12b1775186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading CSVs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load csv files.\n",
    "df_raw = load_csv_files(RAW_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ac7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25,019 rows from ../dataset/raw_docs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>_site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´</td>\n",
       "      <td>&lt;p&gt;‚ÄúI have never thought that I can do importa...</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´</td>\n",
       "      <td>&lt;p&gt;We spoke to Heghine for a long time and she...</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never have I thought</td>\n",
       "      <td>&lt;p&gt;‚ÄúI have never thought that I can do importa...</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Never have I thought</td>\n",
       "      <td>&lt;p&gt;We spoke to Heghine for a long time and she...</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘±’≤’•’ø’∂’•÷Ä’´’∂ ’∫’°’ø÷Ä’°’Ω’ø ’§’∫÷Ä’∏÷Å` ’Ω’°’∞’¥’°’∂’°’¥’•÷Ä’± ’£’µ’∏÷Ç’≤’∏÷Ç’¥</td>\n",
       "      <td>&lt;p&gt;’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´...</td>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  \\\n",
       "0                           ‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´   \n",
       "1                           ‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´   \n",
       "2                           Never have I thought   \n",
       "3                           Never have I thought   \n",
       "4  ‘±’≤’•’ø’∂’•÷Ä’´’∂ ’∫’°’ø÷Ä’°’Ω’ø ’§’∫÷Ä’∏÷Å` ’Ω’°’∞’¥’°’∂’°’¥’•÷Ä’± ’£’µ’∏÷Ç’≤’∏÷Ç’¥   \n",
       "\n",
       "                                                text  \\\n",
       "0  <p>‚ÄúI have never thought that I can do importa...   \n",
       "1  <p>We spoke to Heghine for a long time and she...   \n",
       "2  <p>‚ÄúI have never thought that I can do importa...   \n",
       "3  <p>We spoke to Heghine for a long time and she...   \n",
       "4  <p>’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´...   \n",
       "\n",
       "                          _site  \n",
       "0  armenia__textcontent_article  \n",
       "1  armenia__textcontent_article  \n",
       "2  armenia__textcontent_article  \n",
       "3  armenia__textcontent_article  \n",
       "4  armenia__textcontent_article  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loaded {len(df_raw):,} rows from {RAW_DOCS}\")\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412f68",
   "metadata": {},
   "source": [
    "Removed duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aff9db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Removed 3,174 duplicate texts. Remaining: 21,845 rows.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates.\n",
    "df_nodup, df_dups, n_dups = dedupe_by_text(df_raw)\n",
    "print(f\"üîÅ Removed {n_dups:,} duplicate texts. Remaining: {len(df_nodup):,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2554005",
   "metadata": {},
   "source": [
    "Stored removed records in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cba6cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a log of removed duplicates for auditing\n",
    "df_dups.to_parquet(\"../logs/list-of-removed-duplicates-from-raw-inputs.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf05a1",
   "metadata": {},
   "source": [
    "Pass raw data to text processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd71f17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fd750956f9401a9a8428f348110617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/6r/x82plb356ylfzf1n3t44k5_40000gn/T/jieba.cache\n",
      "Loading model cost 0.636 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "df_processed = process_dataframe_batched(df_nodup, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96556a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed rows: 21,845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´</td>\n",
       "      <td>&lt;p&gt;‚ÄúI have never thought that I can do importa...</td>\n",
       "      <td>\"I have never thought that I can do important ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[\"I have never thought that I can do important...</td>\n",
       "      <td>[\", I, have, never, thought, that, I, can, do,...</td>\n",
       "      <td>9</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´</td>\n",
       "      <td>&lt;p&gt;We spoke to Heghine for a long time and she...</td>\n",
       "      <td>We spoke to Heghine for a long time and she of...</td>\n",
       "      <td>en</td>\n",
       "      <td>[We spoke to Heghine for a long time and she o...</td>\n",
       "      <td>[We, spoke, to, Heghine, for, a, long, time, a...</td>\n",
       "      <td>13</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>armenia__textcontent_article</td>\n",
       "      <td>‘±’≤’•’ø’∂’•÷Ä’´’∂ ’∫’°’ø÷Ä’°’Ω’ø ’§’∫÷Ä’∏÷Å` ’Ω’°’∞’¥’°’∂’°’¥’•÷Ä’± ’£’µ’∏÷Ç’≤’∏÷Ç’¥</td>\n",
       "      <td>&lt;p&gt;’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´...</td>\n",
       "      <td>’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´ VI...</td>\n",
       "      <td>hy</td>\n",
       "      <td>[’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´ V...</td>\n",
       "      <td>[’è’°’æ’∏÷Ç’∑’´, ’¥’°÷Ä’¶’´, ’Ü’•÷Ä÷Ñ’´’∂, ‘æ’°’≤’Ø’°’æ’°’∂, ’£’µ’∏÷Ç’≤’´, ’§’∫÷Ä...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           site  \\\n",
       "0  armenia__textcontent_article   \n",
       "1  armenia__textcontent_article   \n",
       "2  armenia__textcontent_article   \n",
       "\n",
       "                                           title  \\\n",
       "0                           ‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´   \n",
       "1                           ‘µ÷Ä’¢’•÷Ñ ’π’ß’´ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’´   \n",
       "2  ‘±’≤’•’ø’∂’•÷Ä’´’∂ ’∫’°’ø÷Ä’°’Ω’ø ’§’∫÷Ä’∏÷Å` ’Ω’°’∞’¥’°’∂’°’¥’•÷Ä’± ’£’µ’∏÷Ç’≤’∏÷Ç’¥   \n",
       "\n",
       "                                                text  \\\n",
       "0  <p>‚ÄúI have never thought that I can do importa...   \n",
       "1  <p>We spoke to Heghine for a long time and she...   \n",
       "2  <p>’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´...   \n",
       "\n",
       "                                          clean_text lang  \\\n",
       "0  \"I have never thought that I can do important ...   en   \n",
       "1  We spoke to Heghine for a long time and she of...   en   \n",
       "2  ’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´ VI...   hy   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [\"I have never thought that I can do important...   \n",
       "1  [We spoke to Heghine for a long time and she o...   \n",
       "2  [’è’°’æ’∏÷Ç’∑’´ ’¥’°÷Ä’¶’´ ’Ü’•÷Ä÷Ñ’´’∂ ‘æ’°’≤’Ø’°’æ’°’∂ ’£’µ’∏÷Ç’≤’´ ’§’∫÷Ä’∏÷Å’´ V...   \n",
       "\n",
       "                                              tokens  n_sentences  n_tokens  \n",
       "0  [\", I, have, never, thought, that, I, can, do,...            9       260  \n",
       "1  [We, spoke, to, Heghine, for, a, long, time, a...           13       357  \n",
       "2  [’è’°’æ’∏÷Ç’∑’´, ’¥’°÷Ä’¶’´, ’Ü’•÷Ä÷Ñ’´’∂, ‘æ’°’≤’Ø’°’æ’°’∂, ’£’µ’∏÷Ç’≤’´, ’§’∫÷Ä...            1        96  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processed row.\n",
    "print(f\"Processed rows: {len(df_processed):,}\")\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886e18c",
   "metadata": {},
   "source": [
    "Save processed data file to shared data library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba898bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../shared-data-library/out/df_processed.parquet'), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_processed.to_parquet(OUT_PARQ, index=False, compression=\"zstd\")\n",
    "\n",
    "if OUT_JSONL:\n",
    "    OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in df_processed.to_dict(orient=\"records\"):\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "OUT_PARQ, OUT_JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aa14208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± Notebook executed in 5.57 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è± Notebook executed in %.2f minutes\" % ((time.time()-nb_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1d9cf",
   "metadata": {},
   "source": [
    "*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
