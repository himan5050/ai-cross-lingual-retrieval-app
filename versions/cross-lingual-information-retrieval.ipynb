{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fcbb49",
   "metadata": {},
   "source": [
    "# <b>Cross-lingual information retrieval</b>: <b>Pipeline workflows</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b48e9b",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae5464",
   "metadata": {},
   "source": [
    "## <b>Project preparation steps</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d42a7e",
   "metadata": {},
   "source": [
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f18b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import emoji\n",
    "import math\n",
    "\n",
    "# Nltk libraries for text cleaning and processing.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import stopwordsiso as stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Import libraries for text cleaning.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Thread pooling.\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "# Import system specific libraries.\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import fast text library for language detection.\n",
    "import fasttext\n",
    "\n",
    "# Import libraries for performance evaluation and measurements.\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Import FAISS library for indexing embedded vectors.\n",
    "import faiss\n",
    "\n",
    "# Sentence transformer based models.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import pickle for saving and loading objects.\n",
    "import pickle\n",
    "\n",
    "# Import sqlite3 library for storing metadata.\n",
    "import sqlite3\n",
    "\n",
    "# Ignore future and deprecated warnings to get cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Logs.\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acc282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/himanshusharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/himanshusharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/himanshusharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the required nltk packages.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4508e15",
   "metadata": {},
   "source": [
    "### Configuration logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6326b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"pipeline.log\",         # log file name\n",
    "    level=logging.INFO,              # logging level (INFO, DEBUG, ERROR)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # log format\n",
    "    filemode=\"w\"                     # overwrite log file each run (\"a\" to append)\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8e426",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19e9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIRECTORY_PATH = \"config\"\n",
    "DATASET_DIRECTORY_PATH = \"datasets\"\n",
    "DATA_DIRECTORY_PATH = \"data\"\n",
    "MULTILINGUAL_DOCUMENTS_DIRECTORY_PATH = \"datasets/multilingual_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e023c",
   "metadata": {},
   "source": [
    "### Helping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d1fd7",
   "metadata": {},
   "source": [
    "#### 1. Load configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781f8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project specific configuration file.\n",
    "def load_config(filename):\n",
    "    config_file_path = f\"{CONFIG_DIRECTORY_PATH}/{filename}.yml\"\n",
    "    with open(config_file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Return config file.\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e16d0",
   "metadata": {},
   "source": [
    "#### 2. Get Language detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb4680f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained language detection model.\n",
    "def get_langauge_detection_model(language_detection_config):\n",
    "    model = language_detection_config['model']\n",
    "    pre_trained_model_filepath = f\"{DATA_DIRECTORY_PATH}/{model}\"\n",
    "    if not os.path.exists(pre_trained_model_filepath):\n",
    "        raise FileNotFoundError(f\"{pre_trained_model_filepath} not found. Download it from model's website.\")\n",
    "    else:\n",
    "        return fasttext.load_model(pre_trained_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fba2c3",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b86b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configured data.\n",
    "site_metadata_config = load_config('sites-metadata');\n",
    "project_config = load_config('project');\n",
    "\n",
    "# Load fast track model for language detection.\n",
    "fast_track_language_detection_model = get_langauge_detection_model(project_config['language_detection']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6cfc4",
   "metadata": {},
   "source": [
    "### 3. Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861a30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    return project_config['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565633f2",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757eb3f",
   "metadata": {},
   "source": [
    "# <b>Text preprocessing pipeline</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9b674",
   "metadata": {},
   "source": [
    "### Helper functions to preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f4272",
   "metadata": {},
   "source": [
    "#### 1. Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a5bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text to remove html formattings, emojis, puntuations and normalize spaces.\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove html formats.\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove emojis.\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Remove puntuations.\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    \n",
    "    # Remove normalize spaces.\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Remove url links.\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082fd96",
   "metadata": {},
   "source": [
    "#### 2. Text normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7b5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text.\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0fedc",
   "metadata": {},
   "source": [
    "#### 3. Tokenization and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d44205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts in sentences and words, and remove stopwords.\n",
    "def tokenize_and_filter(row, axis = 1):\n",
    "    text = row['cleaned_text']\n",
    "    lang = row['language']\n",
    "\n",
    "    # Sentence tokenize.\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Word tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords if available for that language.\n",
    "    if stopwords.has_lang(lang):\n",
    "        sw = stopwords.stopwords(lang)\n",
    "        words = [w for w in words if w not in sw]\n",
    "\n",
    "    return pd.Series({\"sentences\": sentences, \"tokens\": words})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e79537",
   "metadata": {},
   "source": [
    "#### 4. Comibined all processeing steps in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f29f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Text Cleaning ----------------\n",
    "def process_text(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2acfad",
   "metadata": {},
   "source": [
    "#### 4. Language detection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386b36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Single-Batch Language Detection ----------------\n",
    "def detect_language_batch(text_batch):\n",
    "    labels, _ = fast_track_language_detection_model.predict(text_batch, k=1)\n",
    "    return [lbl[0].replace(\"__label__\", \"\") if lbl else \"unknown\" for lbl in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503583d",
   "metadata": {},
   "source": [
    "4. Thread based batch language detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "846a0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Threaded Batch Language Detection ----------------\n",
    "def batch_detect_language_parallel(texts, batch_size=1000):\n",
    "    chunks = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    languages = []\n",
    "\n",
    "    with Pool() as pool:  # ThreadPool\n",
    "        for batch_result in tqdm(pool.imap(detect_language_batch, chunks), total=len(chunks), desc=\"Language Detection\"):\n",
    "            languages.extend(batch_result)\n",
    "    \n",
    "    return languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77e867",
   "metadata": {},
   "source": [
    "#### 5. Preprocessing single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae60313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Preprocess Single DataFrame ----------------\n",
    "def preprocess_dataframe_parallel(df, metadata):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a text column, returns new Dataframe with \n",
    "    cleaned, normalized, language, tokens.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    text_column = metadata['text_column']\n",
    "    batch_size = metadata['batch_size']\n",
    "    \n",
    "    # Remove duplicates\n",
    "    tqdm.pandas(desc=\"Removing duplicates....\")\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    duplicates_removed = initial_count - len(df)\n",
    "    \n",
    "    # Clean text with progress bar\n",
    "    tqdm.pandas(desc=\"Cleaning text....\")\n",
    "    df['cleaned_text'] = df[text_column].progress_apply(process_text)\n",
    "\n",
    "    \n",
    "    # Remove empty cleaned text\n",
    "    df = df[df['cleaned_text'].notna()].reset_index(drop=True)\n",
    "    \n",
    "    # Detect languages in parallel\n",
    "    texts = df['cleaned_text'].tolist()\n",
    "    df['language'] = batch_detect_language_parallel(texts, batch_size=batch_size)\n",
    "\n",
    "    # Remove unknown languages\n",
    "    df = df[df['language'] != \"unknown\"].reset_index(drop=True)\n",
    "\n",
    "    # tokenization, processin.\n",
    "    tqdm.pandas(desc=\"Tokenizing text....\")\n",
    "    df_tokens = df.progress_apply(tokenize_and_filter, axis = 1)\n",
    "    df = df.join(df_tokens)\n",
    "    \n",
    "    # Capture site stats\n",
    "    site_stats = {\n",
    "        \"total_rows\": initial_count,\n",
    "        \"duplicates_removed\": duplicates_removed,\n",
    "        \"rows_kept\": len(df),\n",
    "        \"languages_detected\": df['language'].unique().tolist()\n",
    "    }\n",
    "    \n",
    "    return df, site_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf16c1",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21613c",
   "metadata": {},
   "source": [
    "# <b> Data preparation pipeline.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19652182",
   "metadata": {},
   "source": [
    "### Multiple csv files reading and processing with summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b3163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Multi-Site CSV Pipeline with Summary ----------------\n",
    "def process_all_sites_with_summary(metadata):\n",
    "\n",
    "    # Get list of sites.\n",
    "    sites = site_metadata_config.get('sites', [])\n",
    "\n",
    "    # Prepare return variables.\n",
    "    all_dfs = []\n",
    "    summary_list = []\n",
    "\n",
    "    for site_csv in sites:\n",
    "        csv_directory = f\"{MULTILINGUAL_DOCUMENTS_DIRECTORY_PATH}/{site_csv}\"\n",
    "        print(csv_directory)\n",
    "\n",
    "        # ---------------- Get list of CSV files ----------------\n",
    "        csv_files = glob.glob(os.path.join(csv_directory, \"*.csv\"))\n",
    "        logger.info(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "        # ---------------- Read all CSVs and combine ----------------\n",
    "        for file in csv_files:\n",
    "            if os.path.exists(file):\n",
    "                input_df = pd.read_csv(file)\n",
    "                df = input_df.copy()\n",
    "                logger.info(f\"[INFO] Processing site: {site_csv} ({len(df)} rows)\")\n",
    "\n",
    "                # Trigger cleaning of dataframes.\n",
    "                df_cleaned, site_stats = preprocess_dataframe_parallel(df, metadata)\n",
    "                logger.info(f\"[INFO] Done {site_csv}: {site_stats['duplicates_removed']} duplicates removed, {site_stats['rows_kept']} rows kept\")\n",
    "                site_stats[\"site\"] = site_csv\n",
    "                summary_list.append(site_stats)\n",
    "                all_dfs.append(df_cleaned)\n",
    "            else:\n",
    "                \n",
    "                logger.info(f\"[WARNING] File not found: {file}\")\n",
    "    \n",
    "    # Merge all cleaned DataFrames\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        merged_df = merged_df.drop_duplicates(subset=[metadata['text_column'], 'cleaned_text']).reset_index(drop=True)\n",
    "        logger.info(f\"[INFO] Merged DataFrame contains {len(merged_df)} unique rows after deduplication\")\n",
    "        \n",
    "        # Save if requested\n",
    "        save_path = f\"{DATA_DIRECTORY_PATH}/{metadata['processed_file_name']}.csv\"\n",
    "        save_format = metadata['data_file_format']\n",
    "        if save_path:\n",
    "            if save_format.lower() == \"csv\":\n",
    "                merged_df.to_csv(save_path, index=False)\n",
    "            elif save_format.lower() == \"parquet\":\n",
    "                merged_df.to_parquet(save_path, index=False)\n",
    "            else:\n",
    "                print(f\"[WARNING] Unknown save_format '{save_format}'. Skipping save.\")\n",
    "            logger.info(f\"[INFO] Saved merged DataFrame to {save_path}\")\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_list)\n",
    "        print(\"\\n[INFO] Site Summary Table:\")\n",
    "        print(summary_df)\n",
    "        \n",
    "        return merged_df, summary_df\n",
    "    else:\n",
    "        print(\"[INFO] No valid data found in any site CSVs.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62411833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/multilingual_documents/india\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text....: 100%|██████████| 511/511 [00:00<00:00, 611.24it/s]\n",
      "/var/folders/6r/x82plb356ylfzf1n3t44k5_40000gn/T/ipykernel_99110/1627730885.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = df[text_column].progress_apply(process_text)\n",
      "Language Detection: 100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "Tokenizing text....: 100%|██████████| 511/511 [00:00<00:00, 1030.97it/s]\n",
      "Cleaning text....: 100%|██████████| 189/189 [00:01<00:00, 143.85it/s]\n",
      "Language Detection: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Tokenizing text....: 100%|██████████| 189/189 [00:00<00:00, 399.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Site Summary Table:\n",
      "   total_rows  duplicates_removed  rows_kept  \\\n",
      "0         609                  98        511   \n",
      "1         189                   0        189   \n",
      "\n",
      "                 languages_detected   site  \n",
      "0  [en, es, fr, sl, sw, zh, pl, el]  india  \n",
      "1                          [en, es]  india  \n"
     ]
    }
   ],
   "source": [
    "# Get the list of sites and candidate languages configurations.\n",
    "metadata = get_metadata()\n",
    "\n",
    "# Trigger pipeline.\n",
    "processed_df, summary_df = process_all_sites_with_summary(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209cf82",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa6443",
   "metadata": {},
   "source": [
    "#### Sites\n",
    "1. Global (https://www.unicef.org/)\n",
    "2. Armenia (https://www.unicef.org/armenia/)\n",
    "3. Bangladesh (https://www.unicef.org/bangladesh/)\n",
    "4. Cambodia (https://www.unicef.org/cambodia/)\n",
    "5. China (https://www.unicef.org/china/)\n",
    "6. ECA (https://www.unicef.org/eca/)\n",
    "7. India (https://www.unicef.org/india/)\n",
    "8. Myanmar (https://www.unicef.org/myanmar)\n",
    "9. Peru (https://www.unicef.org/peru/)\n",
    "10. Vietnam (https://www.unicef.org/vietnam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb51f8",
   "metadata": {},
   "source": [
    "#### Low level Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6e2fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbef921",
   "metadata": {},
   "source": [
    "#### Collect text data of press releases and articles from all candidate sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a460b51",
   "metadata": {},
   "source": [
    "#### Get CSV data into dataframe and execute pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f869c1",
   "metadata": {},
   "source": [
    "1. Read data from csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe58d53",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964930b",
   "metadata": {},
   "source": [
    "## Load processed and cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aed5e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{DATA_DIRECTORY_PATH}/{project_config['metadata']['processed_file_name']}.csv\"\n",
    "df = pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129fbee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>language</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A new start to life for little Durga</td>\n",
       "      <td>&lt;p&gt;Little Durga pinches her brother Vikram and...</td>\n",
       "      <td>little durga pinches her brother vikram and qu...</td>\n",
       "      <td>en</td>\n",
       "      <td>['little durga pinches her brother vikram and ...</td>\n",
       "      <td>['durga', 'pinches', 'brother', 'vikram', 'dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A new start to life for little Durga</td>\n",
       "      <td>&lt;p&gt;And Manju’s happiness is for a reason. Durg...</td>\n",
       "      <td>and manju s happiness is for a reason durga s ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['and manju s happiness is for a reason durga ...</td>\n",
       "      <td>['manju', 'happiness', 'reason', 'durga', 'sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new start to life for little Durga</td>\n",
       "      <td>&lt;h4&gt;A new beginning&lt;/h4&gt;\\r\\n\\r\\n&lt;p&gt;Durga was b...</td>\n",
       "      <td>a new beginning durga was born wih low birth w...</td>\n",
       "      <td>en</td>\n",
       "      <td>['a new beginning durga was born wih low birth...</td>\n",
       "      <td>['durga', 'born', 'wih', 'birth', 'weight', '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A new start to life for little Durga</td>\n",
       "      <td>&lt;p&gt;Close to 39 percent children under five yea...</td>\n",
       "      <td>close to 39 percent children under five years ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['close to 39 percent children under five year...</td>\n",
       "      <td>['close', 'percent', 'children', 'age', 'malno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A new start to life for little Durga</td>\n",
       "      <td>&lt;p&gt;Manju made sure that Durga was regular to t...</td>\n",
       "      <td>manju made sure that durga was regular to the ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['manju made sure that durga was regular to th...</td>\n",
       "      <td>['manju', 'durga', 'regular', 'anganwadi', 'ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Department of Mass Communication and Journalis...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;HYDERABAD, India, 22 May 2025—Creat...</td>\n",
       "      <td>hyderabad india 22 may 2025 creating safer roa...</td>\n",
       "      <td>en</td>\n",
       "      <td>['hyderabad india 22 may 2025 creating safer r...</td>\n",
       "      <td>['hyderabad', 'india', '22', '2025', 'creating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Radio Professionals Come Together To Discuss I...</td>\n",
       "      <td>&lt;p&gt;NEW DELHI, 3 June 2025 – India’s top radio ...</td>\n",
       "      <td>new delhi 3 june 2025 india s top radio voices...</td>\n",
       "      <td>en</td>\n",
       "      <td>['new delhi 3 june 2025 india s top radio voic...</td>\n",
       "      <td>['delhi', '3', 'june', '2025', 'india', 'radio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>अंतरराष्ट्रीय खेल दिवस और विश्व पर्यावरण दिवस ...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;नई दिल्ली, 3 जून 2025 –&lt;/strong&gt; ऑल...</td>\n",
       "      <td>nii d l l 3 j n 2025 oNl i dd y r dd y n j eph...</td>\n",
       "      <td>es</td>\n",
       "      <td>['nii d l l 3 j n 2025 oNl i dd y r dd y n j e...</td>\n",
       "      <td>['nii', '2025', 'oNl', 'dd', 'dd', 'ephem', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>GoI and UNICEF: Peer-Support Critical for Adol...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;BHOPAL, 22 July 2025 –&lt;/strong&gt; To ...</td>\n",
       "      <td>bhopal 22 july 2025 to strengthen the support ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['bhopal 22 july 2025 to strengthen the suppor...</td>\n",
       "      <td>['bhopal', '22', 'july', '2025', 'strengthen',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>भारत सरकार और यूनिसेफ: किशोरों की मानसिक सेहत ...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;भोपाल, 22&amp;nbsp;जुलाई 2025&lt;/strong&gt; ...</td>\n",
       "      <td>bh p l 22 j l ii 2025 d sh m k sh r aur y v o ...</td>\n",
       "      <td>es</td>\n",
       "      <td>['bh p l 22 j l ii 2025 d sh m k sh r aur y v ...</td>\n",
       "      <td>['bh', '22', 'ii', '2025', 'sh', 'sh', 'aur', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                 A new start to life for little Durga   \n",
       "1                 A new start to life for little Durga   \n",
       "2                 A new start to life for little Durga   \n",
       "3                 A new start to life for little Durga   \n",
       "4                 A new start to life for little Durga   \n",
       "..                                                 ...   \n",
       "695  Department of Mass Communication and Journalis...   \n",
       "696  Radio Professionals Come Together To Discuss I...   \n",
       "697  अंतरराष्ट्रीय खेल दिवस और विश्व पर्यावरण दिवस ...   \n",
       "698  GoI and UNICEF: Peer-Support Critical for Adol...   \n",
       "699  भारत सरकार और यूनिसेफ: किशोरों की मानसिक सेहत ...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    <p>Little Durga pinches her brother Vikram and...   \n",
       "1    <p>And Manju’s happiness is for a reason. Durg...   \n",
       "2    <h4>A new beginning</h4>\\r\\n\\r\\n<p>Durga was b...   \n",
       "3    <p>Close to 39 percent children under five yea...   \n",
       "4    <p>Manju made sure that Durga was regular to t...   \n",
       "..                                                 ...   \n",
       "695  <p><strong>HYDERABAD, India, 22 May 2025—Creat...   \n",
       "696  <p>NEW DELHI, 3 June 2025 – India’s top radio ...   \n",
       "697  <p><strong>नई दिल्ली, 3 जून 2025 –</strong> ऑल...   \n",
       "698  <p><strong>BHOPAL, 22 July 2025 –</strong> To ...   \n",
       "699  <p><strong>भोपाल, 22&nbsp;जुलाई 2025</strong> ...   \n",
       "\n",
       "                                          cleaned_text language  \\\n",
       "0    little durga pinches her brother vikram and qu...       en   \n",
       "1    and manju s happiness is for a reason durga s ...       en   \n",
       "2    a new beginning durga was born wih low birth w...       en   \n",
       "3    close to 39 percent children under five years ...       en   \n",
       "4    manju made sure that durga was regular to the ...       en   \n",
       "..                                                 ...      ...   \n",
       "695  hyderabad india 22 may 2025 creating safer roa...       en   \n",
       "696  new delhi 3 june 2025 india s top radio voices...       en   \n",
       "697  nii d l l 3 j n 2025 oNl i dd y r dd y n j eph...       es   \n",
       "698  bhopal 22 july 2025 to strengthen the support ...       en   \n",
       "699  bh p l 22 j l ii 2025 d sh m k sh r aur y v o ...       es   \n",
       "\n",
       "                                             sentences  \\\n",
       "0    ['little durga pinches her brother vikram and ...   \n",
       "1    ['and manju s happiness is for a reason durga ...   \n",
       "2    ['a new beginning durga was born wih low birth...   \n",
       "3    ['close to 39 percent children under five year...   \n",
       "4    ['manju made sure that durga was regular to th...   \n",
       "..                                                 ...   \n",
       "695  ['hyderabad india 22 may 2025 creating safer r...   \n",
       "696  ['new delhi 3 june 2025 india s top radio voic...   \n",
       "697  ['nii d l l 3 j n 2025 oNl i dd y r dd y n j e...   \n",
       "698  ['bhopal 22 july 2025 to strengthen the suppor...   \n",
       "699  ['bh p l 22 j l ii 2025 d sh m k sh r aur y v ...   \n",
       "\n",
       "                                                tokens  \n",
       "0    ['durga', 'pinches', 'brother', 'vikram', 'dis...  \n",
       "1    ['manju', 'happiness', 'reason', 'durga', 'sho...  \n",
       "2    ['durga', 'born', 'wih', 'birth', 'weight', '2...  \n",
       "3    ['close', 'percent', 'children', 'age', 'malno...  \n",
       "4    ['manju', 'durga', 'regular', 'anganwadi', 'ce...  \n",
       "..                                                 ...  \n",
       "695  ['hyderabad', 'india', '22', '2025', 'creating...  \n",
       "696  ['delhi', '3', 'june', '2025', 'india', 'radio...  \n",
       "697  ['nii', '2025', 'oNl', 'dd', 'dd', 'ephem', 'n...  \n",
       "698  ['bhopal', '22', 'july', '2025', 'strengthen',...  \n",
       "699  ['bh', '22', 'ii', '2025', 'sh', 'sh', 'aur', ...  \n",
       "\n",
       "[700 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34695b",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5ef5c",
   "metadata": {},
   "source": [
    "## <b>Model Embedding Pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoids fork warning\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class EmbeddingPipeline:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", index_dir=\"./index_store\", batch_size=512, n_workers=4):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index_dir = index_dir\n",
    "        os.makedirs(index_dir, exist_ok=True)\n",
    "        self.index = None\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = None\n",
    "        self.texts = []\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def generate_embeddings_batch(self, texts, save_path=None):\n",
    "        \"\"\"Generate embeddings in batches, normalize for cosine similarity, optionally save to disk.\"\"\"\n",
    "        all_embeddings = []\n",
    "        for start in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[start:start+self.batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "            faiss.normalize_L2(batch_embeddings)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "            print(f\"Processed batch {start}-{start+len(batch_texts)}\")\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        print(f\"Total embeddings shape: {all_embeddings.shape}\")\n",
    "\n",
    "        if save_path:\n",
    "            np.save(save_path, all_embeddings)\n",
    "            print(f\"Saved embeddings to {save_path}\")\n",
    "\n",
    "        self.dim = all_embeddings.shape[1]\n",
    "        self.texts = texts\n",
    "        return all_embeddings\n",
    "\n",
    "    def build_index(self, embeddings, index_type=\"flat\", **kwargs):\n",
    "        \"\"\"Build FAISS index (flat, hnsw, or ivf) with cosine similarity.\"\"\"\n",
    "        if self.dim is None:\n",
    "            self.dim = embeddings.shape[1]\n",
    "\n",
    "        if index_type == \"flat\":\n",
    "            self.index = faiss.IndexFlatIP(self.dim)\n",
    "\n",
    "        elif index_type == \"hnsw\":\n",
    "            M = kwargs.get(\"M\", 16)\n",
    "            efConstruction = kwargs.get(\"efConstruction\", 100)\n",
    "            self.index = faiss.IndexHNSWFlat(self.dim, M, faiss.METRIC_INNER_PRODUCT)\n",
    "            self.index.hnsw.efConstruction = efConstruction\n",
    "\n",
    "        elif index_type == \"ivf\":\n",
    "            nlist = kwargs.get(\"nlist\", 100)\n",
    "            quantizer = faiss.IndexFlatIP(self.dim)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, self.dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "            print(\"Training IVF index...\")\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported index type. Use 'flat', 'hnsw', or 'ivf'.\")\n",
    "\n",
    "        # Add in batches\n",
    "        for i in range(0, embeddings.shape[0], self.batch_size):\n",
    "            self.index.add(embeddings[i:i+self.batch_size])\n",
    "\n",
    "    def save_index(self, name=\"index.faiss\"):\n",
    "        path = os.path.join(self.index_dir, name)\n",
    "        faiss.write_index(self.index, path)\n",
    "        print(f\"Index saved at {path}\")\n",
    "\n",
    "    def load_index(self, name=\"index.faiss\"):\n",
    "        path = os.path.join(self.index_dir, name)\n",
    "        self.index = faiss.read_index(path)\n",
    "        print(f\"Index loaded from {path}\")\n",
    "\n",
    "    def search(self, query, top_k=5, as_df=True):\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        D, I = self.index.search(query_embedding, top_k)\n",
    "\n",
    "        if as_df:\n",
    "            results = []\n",
    "            for rank, (score, idx) in enumerate(zip(D[0], I[0]), start=1):\n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"rank\": rank,\n",
    "                    \"index\": int(idx),\n",
    "                    \"cosine_similarity\": float(score),\n",
    "                    \"text\": self.texts[idx] if self.texts else None\n",
    "                })\n",
    "            return pd.DataFrame(results)\n",
    "        return D, I\n",
    "\n",
    "    def search_batch(self, queries, top_k=5):\n",
    "        query_embeddings = self.model.encode(queries, convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embeddings)\n",
    "        D, I = self.index.search(query_embeddings, top_k)\n",
    "\n",
    "        all_results = []\n",
    "        for q_idx, query in enumerate(queries):\n",
    "            for rank, (score, idx) in enumerate(zip(D[q_idx], I[q_idx]), start=1):\n",
    "                all_results.append({\n",
    "                    \"query\": query,\n",
    "                    \"rank\": rank,\n",
    "                    \"index\": int(idx),\n",
    "                    \"cosine_similarity\": float(score),\n",
    "                    \"text\": self.texts[idx] if self.texts else None\n",
    "                })\n",
    "        return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EmbeddingPipeline(batch_size=1024, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98241080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate embeddings in batches\n",
    "embeddings = pipeline.generate_embeddings_batch(sentences.to_list(), save_path=\"embeddings_100k.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HNSW index using threads\n",
    "pipeline.build_index(embeddings, index_type=\"flat\", M=16, efConstruction=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Save index\n",
    "pipeline.save_index(\"faiss_hnsw_parallel.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702a7d5",
   "metadata": {},
   "source": [
    "### Load saved vector indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d463adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EmbeddingPipeline()\n",
    "pipeline.load(\"large_index.faiss\", \"metadata_large.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a628cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Search batch queries\n",
    "queries = [\"Unicef kids\", \"उम्मीदों की नई सुबह\", \"mundo\"]\n",
    "df_results = pipeline.search_batch(queries, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b531c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf137c",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b0f26",
   "metadata": {},
   "source": [
    "# Query Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f88bd",
   "metadata": {},
   "source": [
    "### Load index for query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later reload:\n",
    "index = faiss.read_index(\"paraphrase-multilingual-MiniLM-L12-v2__indexes.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40208d8",
   "metadata": {},
   "source": [
    "#### Query to indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae76c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query FAISS\n",
    "query = \"situation reports in hindi\"\n",
    "query_vec = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "D, I = index.search(query_vec, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f5260",
   "metadata": {},
   "source": [
    "## Print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200088b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch metadata from SQLite\n",
    "print(\"\\nSearch results:\")\n",
    "I[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c72c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual-semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
